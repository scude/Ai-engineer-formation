{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1️⃣ Chargement du dataset brut\n",
    "\n",
    "Le fichier `training.1600000.processed.noemoticon.csv` contient **1,6 million de tweets annotés** pour l’analyse de sentiment.\n",
    "Les tweets sont étiquetés avec `target = 0` (négatif) ou `4` (positif). Il n'y a pas d’exemple \"neutre\".\n",
    "\n",
    "Nous chargeons ici le jeu de données en spécifiant manuellement les noms de colonnes :\n",
    "- `target` : étiquette du sentiment\n",
    "- `text` : contenu du tweet (champ d'intérêt)\n"
   ],
   "id": "841ad57ab2ca8f54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:07.881130Z",
     "start_time": "2025-08-19T14:03:04.696294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Définir le chemin vers le dataset\n",
    "dataset_path = \"../data/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "# Charger les données (pas d’en-têtes → on les spécifie)\n",
    "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = pd.read_csv(dataset_path, encoding='latin-1', names=columns)\n",
    "\n",
    "# Aperçu des premières lignes\n",
    "df.head()\n"
   ],
   "id": "bbb13c305c63a648",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analyse de la distribution des classes",
   "id": "8841052aed15be74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:08.760555Z",
     "start_time": "2025-08-19T14:03:07.953777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertir les labels en binaire : 0 = négatif, 1 = positif\n",
    "df[\"label\"] = df[\"target\"].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# Comptage brut\n",
    "class_counts = df[\"label\"].value_counts().sort_index()\n",
    "print(\"Répartition des classes :\\n\", class_counts)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Négatif (0)\", \"Positif (1)\"], class_counts, color=[\"tomato\", \"seagreen\"])\n",
    "plt.title(\"Distribution des classes de sentiments\")\n",
    "plt.ylabel(\"Nombre de tweets\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4c50d18fd45d1eab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes :\n",
      " label\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nettoyage des données inutiles",
   "id": "72a266d1ea9071c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:08.947760Z",
     "start_time": "2025-08-19T14:03:08.895086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Valeurs uniques de la colonne 'flag'\n",
    "unique_flags = df[\"flag\"].value_counts()\n",
    "\n",
    "print(\"Valeurs uniques de la colonne 'flag' :\")\n",
    "print(unique_flags)\n"
   ],
   "id": "962d4bc84e580cdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs uniques de la colonne 'flag' :\n",
      "flag\n",
      "NO_QUERY    1600000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:09.034723Z",
     "start_time": "2025-08-19T14:03:08.998078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Suppression des colonnes non utilisées pour la tâche\n",
    "df.drop(columns=[\"date\", \"flag\", \"user\", \"target\"], inplace=True)\n",
    "\n",
    "# Vérification des colonnes restantes\n",
    "df.head()\n"
   ],
   "id": "5b2be7771cfed49d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          ids                                               text  label\n",
       "0  1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...      0\n",
       "1  1467810672  is upset that he can't update his Facebook by ...      0\n",
       "2  1467810917  @Kenichan I dived many times for the ball. Man...      0\n",
       "3  1467811184    my whole body feels itchy and like its on fire       0\n",
       "4  1467811193  @nationwideclass no, it's not behaving at all....      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:09.528783Z",
     "start_time": "2025-08-19T14:03:09.123334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Nombre de NaN\n",
    "nb_nan = df[\"text\"].isna().sum()\n",
    "\n",
    "# Nombre de chaînes vides (ou espaces uniquement)\n",
    "nb_empty = (df[\"text\"].astype(str).str.strip() == \"\").sum()\n",
    "\n",
    "# Total des cas problématiques\n",
    "total_problematic = nb_nan + nb_empty\n",
    "\n",
    "print(f\"📦 Nombre de NaN : {nb_nan}\")\n",
    "print(f\"🈳 Nombre de textes vides : {nb_empty}\")"
   ],
   "id": "5e834b4dea544dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Nombre de NaN : 0\n",
      "🈳 Nombre de textes vides : 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Anonymisation des données\n",
    "\n",
    "Les pseudonymes ne devraient pas apporter de valeur pour classifier les tweets dans notre cas."
   ],
   "id": "6048938fbced2403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:10.680625Z",
     "start_time": "2025-08-19T14:03:09.624215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def anonymize_mentions(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Supprime les mentions @username pour anonymisation RGPD.\n",
    "    \"\"\"\n",
    "    return re.sub(r'@[\\w_]+', '', text).strip()\n",
    "\n",
    "# Application RGPD\n",
    "df[\"anonymized_text\"] = df[\"text\"].apply(anonymize_mentions)\n",
    "\n",
    "# Exemple\n",
    "df[[\"text\", \"anonymized_text\"]].sample(42).values\n"
   ],
   "id": "3ced2bb527632e86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['@rwdave noise-boy antics?  Do I want to ask or know more? ',\n",
       "        'noise-boy antics?  Do I want to ask or know more?'],\n",
       "       ['@Silveysurfer they also removed the family photos &amp; story from the menu.  I loved that part! Still WAY better than Knoxville Italian.',\n",
       "        'they also removed the family photos &amp; story from the menu.  I loved that part! Still WAY better than Knoxville Italian.'],\n",
       "       [\"my laptop died. I'm pissed. I'm sorry but I won't be able to update fanfiction until I get it fixed. Hopefully it doesn't take too long. \",\n",
       "        \"my laptop died. I'm pissed. I'm sorry but I won't be able to update fanfiction until I get it fixed. Hopefully it doesn't take too long.\"],\n",
       "       ['just woke up from my nap and the feeling of making a vid is coming to me man if i only had some &quot;AA&quot; batteries ',\n",
       "        'just woke up from my nap and the feeling of making a vid is coming to me man if i only had some &quot;AA&quot; batteries'],\n",
       "       ['bummer, the bell on my bike broke off ',\n",
       "        'bummer, the bell on my bike broke off'],\n",
       "       [\"@amandafortier I had a guy do that to me on the highway! I didn't drive home. I went to a guy friends house instead. He parked outside. \",\n",
       "        \"I had a guy do that to me on the highway! I didn't drive home. I went to a guy friends house instead. He parked outside.\"],\n",
       "       [\"Disappointed with Ciara's performance with JT on #SNL ... it was just bad.  A good song was ruined\",\n",
       "        \"Disappointed with Ciara's performance with JT on #SNL ... it was just bad.  A good song was ruined\"],\n",
       "       ['@sarahprout that was nice!  Thanks for the link.  ',\n",
       "        'that was nice!  Thanks for the link.'],\n",
       "       [\"That baby bonked her noggin on the ledge thingy by the slider. She actually has a mark.  Wish she'd stop falling over.\",\n",
       "        \"That baby bonked her noggin on the ledge thingy by the slider. She actually has a mark.  Wish she'd stop falling over.\"],\n",
       "       ['@Belchin_Bitch I would be one too but Blip refuses to play for me 95% of the time.    if u know why tell me?',\n",
       "        'I would be one too but Blip refuses to play for me 95% of the time.    if u know why tell me?'],\n",
       "       [\"@JoniRodgers I thought the same thing when I heard about it...but she doesn't deserve you \",\n",
       "        \"I thought the same thing when I heard about it...but she doesn't deserve you\"],\n",
       "       ['@LENNDEVOURS  I hear he might send the code enforcement officer over to confiscate them. ',\n",
       "        'I hear he might send the code enforcement officer over to confiscate them.'],\n",
       "       ['@ShellyKramer I wonder how people get so emotionally damaged that being hateful is the only way they can lift themselves up. ',\n",
       "        'I wonder how people get so emotionally damaged that being hateful is the only way they can lift themselves up.'],\n",
       "       ['@godmademefunkie Mine is still dead.  Thanks for checking!',\n",
       "        'Mine is still dead.  Thanks for checking!'],\n",
       "       ['GOOD MORNING ,IM LATE FOR THE TIMEâ\\x80¦  SO SORRY ',\n",
       "        'GOOD MORNING ,IM LATE FOR THE TIMEâ\\x80¦  SO SORRY'],\n",
       "       [\"NOW school's really starting. I felt a little funny labeling my stuff 2A. I miss 1B \",\n",
       "        \"NOW school's really starting. I felt a little funny labeling my stuff 2A. I miss 1B\"],\n",
       "       [\"I'm being held captive at the casino by my mother. \",\n",
       "        \"I'm being held captive at the casino by my mother.\"],\n",
       "       ['&gt;&gt;Why did the Blonde Keep failing her drivers license test--------&gt;&gt;&gt;&gt;&gt;Everytime they stopped, she jumped in the back seat. ',\n",
       "        '&gt;&gt;Why did the Blonde Keep failing her drivers license test--------&gt;&gt;&gt;&gt;&gt;Everytime they stopped, she jumped in the back seat.'],\n",
       "       ['Apparently UK, if guessitmation is right, should get iPhone 3.0 at roughly 2PM. Bugger ',\n",
       "        'Apparently UK, if guessitmation is right, should get iPhone 3.0 at roughly 2PM. Bugger'],\n",
       "       ['Thank you to everyone expressing concern about MySpace.  I am grateful to still be employed. Sadly, we lost many of our best minds today ',\n",
       "        'Thank you to everyone expressing concern about MySpace.  I am grateful to still be employed. Sadly, we lost many of our best minds today'],\n",
       "       ['sheesh @ fridays when you have to work emea hours ',\n",
       "        'sheesh @ fridays when you have to work emea hours'],\n",
       "       ['@darave I think you should fill us in on details of the night  or at least fill me in on it, hehe',\n",
       "        'I think you should fill us in on details of the night  or at least fill me in on it, hehe'],\n",
       "       ['@dark_jayy fuck you shes not . @Tori_Amelia Hey tori ',\n",
       "        'fuck you shes not .  Hey tori'],\n",
       "       ['@Kogenre crap, im a week out on the geek meet ?  i guess i have been quite distracted the past week, doh #kilaumeet',\n",
       "        'crap, im a week out on the geek meet ?  i guess i have been quite distracted the past week, doh #kilaumeet'],\n",
       "       ['@ the hospital, suffering a really crucial stomach pain ',\n",
       "        '@ the hospital, suffering a really crucial stomach pain'],\n",
       "       [\"ugh! I'm having a massive headache.. \",\n",
       "        \"ugh! I'm having a massive headache..\"],\n",
       "       ['@jadiecakes I want to go to Target too! They need them in Canada. My closest one is probably in Buffalo...almost 3 hrs away. ',\n",
       "        'I want to go to Target too! They need them in Canada. My closest one is probably in Buffalo...almost 3 hrs away.'],\n",
       "       [\"Sailboats on the lake. It's bright sunny but cool and breezy. Perfect. \",\n",
       "        \"Sailboats on the lake. It's bright sunny but cool and breezy. Perfect.\"],\n",
       "       ['@jehankazi is venting to chintan because hes a good listener ',\n",
       "        'is venting to chintan because hes a good listener'],\n",
       "       ['@NixiePixel perform acrobatics and a hapsicord solo for the Queen. ',\n",
       "        'perform acrobatics and a hapsicord solo for the Queen.'],\n",
       "       ['@gudze to your first post, i play WoW ',\n",
       "        'to your first post, i play WoW'],\n",
       "       [\"@tommcfly http://www.twitpic.com/4fu98 last year's gig, remember? so bad you couldn't get it on stage this year \",\n",
       "        \"http://www.twitpic.com/4fu98 last year's gig, remember? so bad you couldn't get it on stage this year\"],\n",
       "       [\"@Katie_Locker dude it's raining it's so depressing I want a sunny day for the dinner  wah !!!\",\n",
       "        \"dude it's raining it's so depressing I want a sunny day for the dinner  wah !!!\"],\n",
       "       ['Also, French exam tomorrow, double fuck ',\n",
       "        'Also, French exam tomorrow, double fuck'],\n",
       "       ['sorry i for got the @ thing ', 'sorry i for got the @ thing'],\n",
       "       ['@CarePathways Try green tea, it is wonderful, especially at night. ',\n",
       "        'Try green tea, it is wonderful, especially at night.'],\n",
       "       [\"Where's my wallet? \", \"Where's my wallet?\"],\n",
       "       [\"@MissLaura317 Well it's not raining.. just cloudy and windy so yeah. PIZZA flavorrr yesss. And sad  sorry you couldn't go.\",\n",
       "        \"Well it's not raining.. just cloudy and windy so yeah. PIZZA flavorrr yesss. And sad  sorry you couldn't go.\"],\n",
       "       ['The most annoying mesquito bites are the ones on my eyelids or lips. ',\n",
       "        'The most annoying mesquito bites are the ones on my eyelids or lips.'],\n",
       "       ['@seankingston hit the club ', 'hit the club'],\n",
       "       ['@monchalee No crazy diets; exercise is much better. But more time consuming. ',\n",
       "        'No crazy diets; exercise is much better. But more time consuming.'],\n",
       "       [\"Got all ready to go out but had to cancel. stupid headache wouldn't go away...  http://short.to/cp3j\",\n",
       "        \"Got all ready to go out but had to cancel. stupid headache wouldn't go away...  http://short.to/cp3j\"]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:11.439089Z",
     "start_time": "2025-08-19T14:03:10.985857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Nombre de NaN\n",
    "nb_nan = df[\"anonymized_text\"].isna().sum()\n",
    "\n",
    "# Nombre de chaînes vides (ou espaces uniquement)\n",
    "nb_empty = (df[\"anonymized_text\"].astype(str).str.strip() == \"\").sum()\n",
    "\n",
    "# Total des cas problématiques\n",
    "total_problematic = nb_nan + nb_empty\n",
    "\n",
    "print(f\"📦 Nombre de NaN : {nb_nan}\")\n",
    "print(f\"🈳 Nombre de textes vides : {nb_empty}\")\n",
    "print(f\"❗ Total à traiter : {total_problematic} / {len(df)} ({round(total_problematic / len(df) * 100, 4)}%)\")\n"
   ],
   "id": "86c43767f864f07d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Nombre de NaN : 0\n",
      "🈳 Nombre de textes vides : 2717\n",
      "❗ Total à traiter : 2717 / 1600000 (0.1698%)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Certains tweet n'étaient constitué que d'une citation d'un pseudonyme et se retrouvent donc vide, il faut les traiter",
   "id": "dd1d4e0dd626ef87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:12.038747Z",
     "start_time": "2025-08-19T14:03:11.447362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Supprime les lignes où 'text' est NaN ou vide après strip\n",
    "df = df[~df[\"anonymized_text\"].isna()]\n",
    "df = df[df[\"anonymized_text\"].str.strip() != \"\"]\n"
   ],
   "id": "40917a8990da6e8f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> On vérifie de nouveau la répartition des classes après suppression d'individus",
   "id": "6b5bef6fb52edb00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:12.057203Z",
     "start_time": "2025-08-19T14:03:12.048023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Comptage brut\n",
    "class_counts = df[\"label\"].value_counts().sort_index()\n",
    "print(\"Répartition des classes :\\n\", class_counts)"
   ],
   "id": "f9945271f72d4606",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes :\n",
      " label\n",
      "0    798519\n",
      "1    798764\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyses des données en fonction de la classe",
   "id": "e1f063c762fd9e3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:20.907126Z",
     "start_time": "2025-08-19T14:03:12.096982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utiliser le texte anonymisé\n",
    "text_col = \"anonymized_text\"\n",
    "\n",
    "# Calcul de métriques sur les tweets\n",
    "df[\"char_len\"] = df[text_col].str.len()\n",
    "df[\"word_count\"] = df[text_col].str.split().apply(len)\n",
    "df[\"uppercase_count\"] = df[text_col].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "\n",
    "# Résumé statistique\n",
    "summary = df[[\"char_len\", \"word_count\", \"uppercase_count\"]].describe().T\n",
    "summary[\"median\"] = df[[\"char_len\", \"word_count\", \"uppercase_count\"]].median()\n",
    "\n",
    "summary\n"
   ],
   "id": "11ba911c91f7115",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     count       mean        std  min   25%   50%   75%  \\\n",
       "char_len         1597283.0  67.442342  36.137421  1.0  37.0  63.0  96.0   \n",
       "word_count       1597283.0  12.720478   6.936011  1.0   7.0  12.0  18.0   \n",
       "uppercase_count  1597283.0   2.818777   5.045927  0.0   0.0   2.0   3.0   \n",
       "\n",
       "                   max  median  \n",
       "char_len         360.0    63.0  \n",
       "word_count        64.0    12.0  \n",
       "uppercase_count  131.0     2.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>char_len</th>\n",
       "      <td>1597283.0</td>\n",
       "      <td>67.442342</td>\n",
       "      <td>36.137421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>1597283.0</td>\n",
       "      <td>12.720478</td>\n",
       "      <td>6.936011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uppercase_count</th>\n",
       "      <td>1597283.0</td>\n",
       "      <td>2.818777</td>\n",
       "      <td>5.045927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> En général un tweet fait 63 caractères et comporte 12 mots",
   "id": "5a3bd85a60be98a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:21.133284Z",
     "start_time": "2025-08-19T14:03:21.083782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Moyenne des longueurs de tweet par label\n",
    "avg_lengths = df.groupby(\"label\")[[\"char_len\", \"word_count\"]].mean().rename(\n",
    "    columns={\"char_len\": \"avg_char_len\", \"word_count\": \"avg_word_count\"}\n",
    ")\n",
    "\n",
    "# Affichage\n",
    "avg_lengths\n"
   ],
   "id": "997149e9247a7fc3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       avg_char_len  avg_word_count\n",
       "label                              \n",
       "0         68.867008       13.222625\n",
       "1         66.018114       12.218485"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_char_len</th>\n",
       "      <th>avg_word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.867008</td>\n",
       "      <td>13.222625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66.018114</td>\n",
       "      <td>12.218485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Les tweets positif comporte en moyenne un mot de plus que les tweet négatif et seulement 2 caractères de plus",
   "id": "66fa5a0f47b6bf43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:28.510357Z",
     "start_time": "2025-08-19T14:03:21.203142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Densité de majuscules = nombre de majuscules / longueur totale (évite division par zéro)\n",
    "df[\"uppercase_ratio\"] = df.apply(\n",
    "    lambda row: row[\"uppercase_count\"] / row[\"char_len\"] if row[\"char_len\"] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Moyenne du ratio par label\n",
    "uppercase_ratio_mean = df.groupby(\"label\")[\"uppercase_ratio\"].mean() * 100\n",
    "uppercase_ratio_mean\n"
   ],
   "id": "5546a24e7a8e835a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    4.177202\n",
       "1    4.805592\n",
       "Name: uppercase_ratio, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Les tweets négatifs comportes légèrement plsu de majuscule en moyenne",
   "id": "e7f10c9a016b7c0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:30.678148Z",
     "start_time": "2025-08-19T14:03:28.743712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Définition des jeux de smileys\n",
    "smiley_sets = {\n",
    "    \"has_neg_smiley\": [\":(\", \":-(\", \":'(\", \">:(\", \"D:\", \":[\"],\n",
    "    \"has_pos_smiley\": [\":)\", \":-)\", \":D\", \":-D\", \"<3\", \":]\"]\n",
    "}\n",
    "\n",
    "# Fonction générique\n",
    "def contains_any(text: str, patterns: list) -> bool:\n",
    "    return any(p in text for p in patterns)\n",
    "\n",
    "# Application pour chaque type de smiley\n",
    "for col_name, smileys in smiley_sets.items():\n",
    "    df[col_name] = df[\"text\"].apply(lambda x: contains_any(x, smileys))\n",
    "\n",
    "# Statistiques par label\n",
    "df.groupby(\"label\")[list(smiley_sets.keys())].mean() * 100\n"
   ],
   "id": "f5accc5f458809fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       has_neg_smiley  has_pos_smiley\n",
       "label                                \n",
       "0            0.271503        0.076016\n",
       "1            0.080875        0.115804"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_neg_smiley</th>\n",
       "      <th>has_pos_smiley</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.271503</td>\n",
       "      <td>0.076016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080875</td>\n",
       "      <td>0.115804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Les tweets négatif comporte plus de smileys negatifs, mais aussi légèrement plus de smileys positif (pouvant traduire du sarcasme)",
   "id": "ba4dc8e7df82e54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:41.034273Z",
     "start_time": "2025-08-19T14:03:30.863660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import string\n",
    "\n",
    "# Fonction pour compter la ponctuation dans un tweet\n",
    "def count_punctuation(text: str) -> int:\n",
    "    return sum(1 for c in text if c in string.punctuation)\n",
    "\n",
    "# Compte du nombre total de ponctuations et de points d'exclamation\n",
    "df[\"punctuation_count\"] = df[\"text\"].apply(count_punctuation)\n",
    "df[\"exclamation_count\"] = df[\"text\"].str.count(\"!\")\n",
    "\n",
    "# Ratio ponctuation / nombre de caractères\n",
    "df[\"punctuation_ratio\"] = df.apply(\n",
    "    lambda row: row[\"punctuation_count\"] / row[\"char_len\"] if row[\"char_len\"] > 0 else 0,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Moyennes par label\n",
    "punct_stats = df.groupby(\"label\")[[\"punctuation_ratio\", \"exclamation_count\"]].mean()\n",
    "punct_stats[\"punctuation_ratio (%)\"] = punct_stats[\"punctuation_ratio\"] * 100\n",
    "punct_stats = punct_stats[[\"punctuation_ratio (%)\", \"exclamation_count\"]]\n",
    "punct_stats\n"
   ],
   "id": "5d48f56d2a11c8a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       punctuation_ratio (%)  exclamation_count\n",
       "label                                          \n",
       "0                   6.004767           0.482431\n",
       "1                   7.201492           0.666930"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>punctuation_ratio (%)</th>\n",
       "      <th>exclamation_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.004767</td>\n",
       "      <td>0.482431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.201492</td>\n",
       "      <td>0.666930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Les Tweets négatif comportent plus de ponctuation en moyenne et très légèrement plus de point d'exclamation.",
   "id": "923015a461843f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:42.898017Z",
     "start_time": "2025-08-19T14:03:41.274373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"allcaps_words\"] = df[\"text\"].apply(lambda x: sum(1 for word in x.split() if word.isupper() and len(word) > 1))\n",
    "df.groupby(\"label\")[[\"allcaps_words\"]].mean()\n"
   ],
   "id": "8291c09347ca5092",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       allcaps_words\n",
       "label               \n",
       "0           0.277466\n",
       "1           0.290962"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allcaps_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.290962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:46.033908Z",
     "start_time": "2025-08-19T14:03:43.084197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction de détection de répétitions de lettres (≥ 3 fois consécutives)\n",
    "def has_repeated_letters(text: str) -> bool:\n",
    "    return bool(re.search(r\"(.)\\1{2,}\", text, flags=re.IGNORECASE))\n",
    "\n",
    "# Application sur le texte brut\n",
    "df[\"has_repeated_letters\"] = df[\"text\"].apply(has_repeated_letters)\n",
    "\n",
    "# Moyenne par label\n",
    "df.groupby(\"label\")[\"has_repeated_letters\"].mean() * 100\n"
   ],
   "id": "ec5d3379726a586d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    29.486211\n",
       "1    26.324421\n",
       "Name: has_repeated_letters, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> Contre toute attente, la répétition de lettre dans un mot est plus importantes pour les tweet positif et traduisent probablement plus la joie que l'énervement",
   "id": "287dda764d7c32b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Export d'un CSV anonimizé",
   "id": "c920f97a23ebb192"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:48.281529Z",
     "start_time": "2025-08-19T14:03:46.244318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sélection minimale des colonnes utiles\n",
    "df_minimal = df[[\"ids\", \"anonymized_text\", \"label\"]].rename(columns={\"anonymized_text\": \"text\"})\n",
    "\n",
    "# Export CSV\n",
    "df_minimal.to_csv(\"../data/clean.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Fichier minimal exporté : data/clean.csv\")\n"
   ],
   "id": "7192929362ae9f35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier minimal exporté : data/clean.csv\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:03:55.860714Z",
     "start_time": "2025-08-19T14:03:48.343266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Chargement du fichier clean.csv\n",
    "df_clean = pd.read_csv(\"../data/clean.csv\")\n",
    "\n",
    "# 1. Séparation train (80%) / temp (20%)\n",
    "df_train, df_temp = train_test_split(\n",
    "    df_clean, test_size=0.2, stratify=df_clean[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# 2. Séparation temp → validation (10%) / test (10%)\n",
    "df_val, df_test = train_test_split(\n",
    "    df_temp, test_size=0.5, stratify=df_temp[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# Vérification des tailles\n",
    "print(f\"Train set : {df_train.shape}\")\n",
    "print(f\"Validation set : {df_val.shape}\")\n",
    "print(f\"Test set : {df_test.shape}\")\n",
    "\n",
    "# Sauvegarde des splits\n",
    "df_train.to_csv(\"../data/train.csv\", index=False)\n",
    "df_val.to_csv(\"../data/val.csv\", index=False)\n",
    "df_test.to_csv(\"../data/test.csv\", index=False)\n",
    "\n",
    "print(\"✅ Fichiers exportés : train.csv, val.csv, test.csv dans /data\")\n"
   ],
   "id": "8d2c3417f416870c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set : (1277826, 3)\n",
      "Validation set : (159728, 3)\n",
      "Test set : (159729, 3)\n",
      "✅ Fichiers exportés : train.csv, val.csv, test.csv dans /data\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Nettoyage des textes pour certains modèles",
   "id": "87b85dcb8d98f1df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:04:01.288091Z",
     "start_time": "2025-08-19T14:03:55.933883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import html\n",
    "from typing import Iterable, List, Literal, Optional\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "except OSError:\n",
    "    # Téléchargement dans le notebook si nécessaire\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "SPACY_STOPWORDS = nlp.Defaults.stop_words\n",
    "\n",
    "# --- Stemming NLTK---\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "_STEMMER: Optional[SnowballStemmer] = None\n",
    "def get_stemmer(lang: str = \"english\") -> SnowballStemmer:\n",
    "    global _STEMMER\n",
    "    if _STEMMER is None:\n",
    "        _STEMMER = SnowballStemmer(lang)\n",
    "    return _STEMMER\n",
    "\n",
    "\n",
    "def basic_cleanup(text: str) -> str:\n",
    "    \"\"\"Nettoyage brut (minuscule, HTML, mentions, URLs, ponctuation, espaces).\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = html.unescape(text)                             # &quot; → \"\n",
    "    text = re.sub(r'@[\\w_]+', ' ', text)                  # @user\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)         # URLs\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)                  # ponctuation → espace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)                      # espaces multiples\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_batch(\n",
    "    texts: Iterable[str],\n",
    "    mode: Literal[\"none\", \"lemma\", \"stem\"] = \"none\",\n",
    "    *,\n",
    "    remove_stopwords: bool = True,\n",
    "    min_len: int = 2,\n",
    "    batch_size: int = 500,\n",
    "    n_process: int = -1,\n",
    "    verbose_every: int = 10000,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Prétraitement par lots avec trois modes:\n",
    "      - mode=\"none\"  : pas de lemmatisation ni stemming (tokens bruts spaCy)\n",
    "      - mode=\"lemma\" : lemmatisation spaCy\n",
    "      - mode=\"stem\"  : stemming Snowball (NLTK)\n",
    "\n",
    "    Params:\n",
    "      remove_stopwords : enlève les stopwords anglais (jeu spaCy)\n",
    "      min_len          : longueur min du token gardé\n",
    "      batch_size       : taille des lots pour nlp.pipe\n",
    "      n_process        : nb de processus spaCy (-1 = auto toutes CPU)\n",
    "      verbose_every    : affichage de progression tous les N docs\n",
    "    \"\"\"\n",
    "    assert mode in {\"none\", \"lemma\", \"stem\"}, \"mode must be 'none' | 'lemma' | 'stem'\"\n",
    "\n",
    "    cleaned_texts = [basic_cleanup(t) for t in texts]\n",
    "    results: List[str] = []\n",
    "\n",
    "    # Prépare le stemmer si nécessaire\n",
    "    stemmer = get_stemmer(\"english\") if mode == \"stem\" else None\n",
    "\n",
    "    print(f\"🚀 Prétraitement ({mode}) sur {len(cleaned_texts)} textes...\")\n",
    "    for i, doc in enumerate(nlp.pipe(cleaned_texts, batch_size=batch_size, n_process=n_process)):\n",
    "        tokens_out = []\n",
    "\n",
    "        for tok in doc:\n",
    "            # filtres de base\n",
    "            if tok.is_space or tok.is_punct or tok.is_digit:\n",
    "                continue\n",
    "\n",
    "            t = tok.text  # valeur par défaut = token brut\n",
    "\n",
    "            if remove_stopwords and t in SPACY_STOPWORDS:\n",
    "                continue\n",
    "            if len(t) < min_len:\n",
    "                continue\n",
    "\n",
    "            # transformation selon le mode\n",
    "            if mode == \"lemma\":\n",
    "                t = tok.lemma_\n",
    "            elif mode == \"stem\":\n",
    "                # On stemme la forme en minuscules (déjà minuscule via basic_cleanup)\n",
    "                t = stemmer.stem(t)\n",
    "\n",
    "            tokens_out.append(t)\n",
    "\n",
    "        results.append(\" \".join(tokens_out))\n",
    "\n",
    "        if verbose_every and (i + 1) % verbose_every == 0:\n",
    "            print(f\"✅ {i + 1}/{len(cleaned_texts)} traités...\")\n",
    "\n",
    "    print(\"🏁 Terminé.\")\n",
    "    return results\n"
   ],
   "id": "89cc0d09889aa096",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:34:57.937517Z",
     "start_time": "2025-08-19T14:04:01.353143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRAIN\n",
    "df_train[\"text\"] = df_train[\"text\"].fillna(\"\").astype(str)\n",
    "df_train[\"text_clean\"] = preprocess_batch(df_train[\"text\"].tolist(), mode=\"none\")\n",
    "df_train[\"text_lemma\"] = preprocess_batch(df_train[\"text\"].tolist(), mode=\"lemma\")\n",
    "df_train[\"text_stem\"]  = preprocess_batch(df_train[\"text\"].tolist(), mode=\"stem\")\n",
    "\n"
   ],
   "id": "ed6d8a99ff4c2dfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Prétraitement (none) sur 1277826 textes...\n",
      "✅ 10000/1277826 traités...\n",
      "✅ 20000/1277826 traités...\n",
      "✅ 30000/1277826 traités...\n",
      "✅ 40000/1277826 traités...\n",
      "✅ 50000/1277826 traités...\n",
      "✅ 60000/1277826 traités...\n",
      "✅ 70000/1277826 traités...\n",
      "✅ 80000/1277826 traités...\n",
      "✅ 90000/1277826 traités...\n",
      "✅ 100000/1277826 traités...\n",
      "✅ 110000/1277826 traités...\n",
      "✅ 120000/1277826 traités...\n",
      "✅ 130000/1277826 traités...\n",
      "✅ 140000/1277826 traités...\n",
      "✅ 150000/1277826 traités...\n",
      "✅ 160000/1277826 traités...\n",
      "✅ 170000/1277826 traités...\n",
      "✅ 180000/1277826 traités...\n",
      "✅ 190000/1277826 traités...\n",
      "✅ 200000/1277826 traités...\n",
      "✅ 210000/1277826 traités...\n",
      "✅ 220000/1277826 traités...\n",
      "✅ 230000/1277826 traités...\n",
      "✅ 240000/1277826 traités...\n",
      "✅ 250000/1277826 traités...\n",
      "✅ 260000/1277826 traités...\n",
      "✅ 270000/1277826 traités...\n",
      "✅ 280000/1277826 traités...\n",
      "✅ 290000/1277826 traités...\n",
      "✅ 300000/1277826 traités...\n",
      "✅ 310000/1277826 traités...\n",
      "✅ 320000/1277826 traités...\n",
      "✅ 330000/1277826 traités...\n",
      "✅ 340000/1277826 traités...\n",
      "✅ 350000/1277826 traités...\n",
      "✅ 360000/1277826 traités...\n",
      "✅ 370000/1277826 traités...\n",
      "✅ 380000/1277826 traités...\n",
      "✅ 390000/1277826 traités...\n",
      "✅ 400000/1277826 traités...\n",
      "✅ 410000/1277826 traités...\n",
      "✅ 420000/1277826 traités...\n",
      "✅ 430000/1277826 traités...\n",
      "✅ 440000/1277826 traités...\n",
      "✅ 450000/1277826 traités...\n",
      "✅ 460000/1277826 traités...\n",
      "✅ 470000/1277826 traités...\n",
      "✅ 480000/1277826 traités...\n",
      "✅ 490000/1277826 traités...\n",
      "✅ 500000/1277826 traités...\n",
      "✅ 510000/1277826 traités...\n",
      "✅ 520000/1277826 traités...\n",
      "✅ 530000/1277826 traités...\n",
      "✅ 540000/1277826 traités...\n",
      "✅ 550000/1277826 traités...\n",
      "✅ 560000/1277826 traités...\n",
      "✅ 570000/1277826 traités...\n",
      "✅ 580000/1277826 traités...\n",
      "✅ 590000/1277826 traités...\n",
      "✅ 600000/1277826 traités...\n",
      "✅ 610000/1277826 traités...\n",
      "✅ 620000/1277826 traités...\n",
      "✅ 630000/1277826 traités...\n",
      "✅ 640000/1277826 traités...\n",
      "✅ 650000/1277826 traités...\n",
      "✅ 660000/1277826 traités...\n",
      "✅ 670000/1277826 traités...\n",
      "✅ 680000/1277826 traités...\n",
      "✅ 690000/1277826 traités...\n",
      "✅ 700000/1277826 traités...\n",
      "✅ 710000/1277826 traités...\n",
      "✅ 720000/1277826 traités...\n",
      "✅ 730000/1277826 traités...\n",
      "✅ 740000/1277826 traités...\n",
      "✅ 750000/1277826 traités...\n",
      "✅ 760000/1277826 traités...\n",
      "✅ 770000/1277826 traités...\n",
      "✅ 780000/1277826 traités...\n",
      "✅ 790000/1277826 traités...\n",
      "✅ 800000/1277826 traités...\n",
      "✅ 810000/1277826 traités...\n",
      "✅ 820000/1277826 traités...\n",
      "✅ 830000/1277826 traités...\n",
      "✅ 840000/1277826 traités...\n",
      "✅ 850000/1277826 traités...\n",
      "✅ 860000/1277826 traités...\n",
      "✅ 870000/1277826 traités...\n",
      "✅ 880000/1277826 traités...\n",
      "✅ 890000/1277826 traités...\n",
      "✅ 900000/1277826 traités...\n",
      "✅ 910000/1277826 traités...\n",
      "✅ 920000/1277826 traités...\n",
      "✅ 930000/1277826 traités...\n",
      "✅ 940000/1277826 traités...\n",
      "✅ 950000/1277826 traités...\n",
      "✅ 960000/1277826 traités...\n",
      "✅ 970000/1277826 traités...\n",
      "✅ 980000/1277826 traités...\n",
      "✅ 990000/1277826 traités...\n",
      "✅ 1000000/1277826 traités...\n",
      "✅ 1010000/1277826 traités...\n",
      "✅ 1020000/1277826 traités...\n",
      "✅ 1030000/1277826 traités...\n",
      "✅ 1040000/1277826 traités...\n",
      "✅ 1050000/1277826 traités...\n",
      "✅ 1060000/1277826 traités...\n",
      "✅ 1070000/1277826 traités...\n",
      "✅ 1080000/1277826 traités...\n",
      "✅ 1090000/1277826 traités...\n",
      "✅ 1100000/1277826 traités...\n",
      "✅ 1110000/1277826 traités...\n",
      "✅ 1120000/1277826 traités...\n",
      "✅ 1130000/1277826 traités...\n",
      "✅ 1140000/1277826 traités...\n",
      "✅ 1150000/1277826 traités...\n",
      "✅ 1160000/1277826 traités...\n",
      "✅ 1170000/1277826 traités...\n",
      "✅ 1180000/1277826 traités...\n",
      "✅ 1190000/1277826 traités...\n",
      "✅ 1200000/1277826 traités...\n",
      "✅ 1210000/1277826 traités...\n",
      "✅ 1220000/1277826 traités...\n",
      "✅ 1230000/1277826 traités...\n",
      "✅ 1240000/1277826 traités...\n",
      "✅ 1250000/1277826 traités...\n",
      "✅ 1260000/1277826 traités...\n",
      "✅ 1270000/1277826 traités...\n",
      "🏁 Terminé.\n",
      "🚀 Prétraitement (lemma) sur 1277826 textes...\n",
      "✅ 10000/1277826 traités...\n",
      "✅ 20000/1277826 traités...\n",
      "✅ 30000/1277826 traités...\n",
      "✅ 40000/1277826 traités...\n",
      "✅ 50000/1277826 traités...\n",
      "✅ 60000/1277826 traités...\n",
      "✅ 70000/1277826 traités...\n",
      "✅ 80000/1277826 traités...\n",
      "✅ 90000/1277826 traités...\n",
      "✅ 100000/1277826 traités...\n",
      "✅ 110000/1277826 traités...\n",
      "✅ 120000/1277826 traités...\n",
      "✅ 130000/1277826 traités...\n",
      "✅ 140000/1277826 traités...\n",
      "✅ 150000/1277826 traités...\n",
      "✅ 160000/1277826 traités...\n",
      "✅ 170000/1277826 traités...\n",
      "✅ 180000/1277826 traités...\n",
      "✅ 190000/1277826 traités...\n",
      "✅ 200000/1277826 traités...\n",
      "✅ 210000/1277826 traités...\n",
      "✅ 220000/1277826 traités...\n",
      "✅ 230000/1277826 traités...\n",
      "✅ 240000/1277826 traités...\n",
      "✅ 250000/1277826 traités...\n",
      "✅ 260000/1277826 traités...\n",
      "✅ 270000/1277826 traités...\n",
      "✅ 280000/1277826 traités...\n",
      "✅ 290000/1277826 traités...\n",
      "✅ 300000/1277826 traités...\n",
      "✅ 310000/1277826 traités...\n",
      "✅ 320000/1277826 traités...\n",
      "✅ 330000/1277826 traités...\n",
      "✅ 340000/1277826 traités...\n",
      "✅ 350000/1277826 traités...\n",
      "✅ 360000/1277826 traités...\n",
      "✅ 370000/1277826 traités...\n",
      "✅ 380000/1277826 traités...\n",
      "✅ 390000/1277826 traités...\n",
      "✅ 400000/1277826 traités...\n",
      "✅ 410000/1277826 traités...\n",
      "✅ 420000/1277826 traités...\n",
      "✅ 430000/1277826 traités...\n",
      "✅ 440000/1277826 traités...\n",
      "✅ 450000/1277826 traités...\n",
      "✅ 460000/1277826 traités...\n",
      "✅ 470000/1277826 traités...\n",
      "✅ 480000/1277826 traités...\n",
      "✅ 490000/1277826 traités...\n",
      "✅ 500000/1277826 traités...\n",
      "✅ 510000/1277826 traités...\n",
      "✅ 520000/1277826 traités...\n",
      "✅ 530000/1277826 traités...\n",
      "✅ 540000/1277826 traités...\n",
      "✅ 550000/1277826 traités...\n",
      "✅ 560000/1277826 traités...\n",
      "✅ 570000/1277826 traités...\n",
      "✅ 580000/1277826 traités...\n",
      "✅ 590000/1277826 traités...\n",
      "✅ 600000/1277826 traités...\n",
      "✅ 610000/1277826 traités...\n",
      "✅ 620000/1277826 traités...\n",
      "✅ 630000/1277826 traités...\n",
      "✅ 640000/1277826 traités...\n",
      "✅ 650000/1277826 traités...\n",
      "✅ 660000/1277826 traités...\n",
      "✅ 670000/1277826 traités...\n",
      "✅ 680000/1277826 traités...\n",
      "✅ 690000/1277826 traités...\n",
      "✅ 700000/1277826 traités...\n",
      "✅ 710000/1277826 traités...\n",
      "✅ 720000/1277826 traités...\n",
      "✅ 730000/1277826 traités...\n",
      "✅ 740000/1277826 traités...\n",
      "✅ 750000/1277826 traités...\n",
      "✅ 760000/1277826 traités...\n",
      "✅ 770000/1277826 traités...\n",
      "✅ 780000/1277826 traités...\n",
      "✅ 790000/1277826 traités...\n",
      "✅ 800000/1277826 traités...\n",
      "✅ 810000/1277826 traités...\n",
      "✅ 820000/1277826 traités...\n",
      "✅ 830000/1277826 traités...\n",
      "✅ 840000/1277826 traités...\n",
      "✅ 850000/1277826 traités...\n",
      "✅ 860000/1277826 traités...\n",
      "✅ 870000/1277826 traités...\n",
      "✅ 880000/1277826 traités...\n",
      "✅ 890000/1277826 traités...\n",
      "✅ 900000/1277826 traités...\n",
      "✅ 910000/1277826 traités...\n",
      "✅ 920000/1277826 traités...\n",
      "✅ 930000/1277826 traités...\n",
      "✅ 940000/1277826 traités...\n",
      "✅ 950000/1277826 traités...\n",
      "✅ 960000/1277826 traités...\n",
      "✅ 970000/1277826 traités...\n",
      "✅ 980000/1277826 traités...\n",
      "✅ 990000/1277826 traités...\n",
      "✅ 1000000/1277826 traités...\n",
      "✅ 1010000/1277826 traités...\n",
      "✅ 1020000/1277826 traités...\n",
      "✅ 1030000/1277826 traités...\n",
      "✅ 1040000/1277826 traités...\n",
      "✅ 1050000/1277826 traités...\n",
      "✅ 1060000/1277826 traités...\n",
      "✅ 1070000/1277826 traités...\n",
      "✅ 1080000/1277826 traités...\n",
      "✅ 1090000/1277826 traités...\n",
      "✅ 1100000/1277826 traités...\n",
      "✅ 1110000/1277826 traités...\n",
      "✅ 1120000/1277826 traités...\n",
      "✅ 1130000/1277826 traités...\n",
      "✅ 1140000/1277826 traités...\n",
      "✅ 1150000/1277826 traités...\n",
      "✅ 1160000/1277826 traités...\n",
      "✅ 1170000/1277826 traités...\n",
      "✅ 1180000/1277826 traités...\n",
      "✅ 1190000/1277826 traités...\n",
      "✅ 1200000/1277826 traités...\n",
      "✅ 1210000/1277826 traités...\n",
      "✅ 1220000/1277826 traités...\n",
      "✅ 1230000/1277826 traités...\n",
      "✅ 1240000/1277826 traités...\n",
      "✅ 1250000/1277826 traités...\n",
      "✅ 1260000/1277826 traités...\n",
      "✅ 1270000/1277826 traités...\n",
      "🏁 Terminé.\n",
      "🚀 Prétraitement (stem) sur 1277826 textes...\n",
      "✅ 10000/1277826 traités...\n",
      "✅ 20000/1277826 traités...\n",
      "✅ 30000/1277826 traités...\n",
      "✅ 40000/1277826 traités...\n",
      "✅ 50000/1277826 traités...\n",
      "✅ 60000/1277826 traités...\n",
      "✅ 70000/1277826 traités...\n",
      "✅ 80000/1277826 traités...\n",
      "✅ 90000/1277826 traités...\n",
      "✅ 100000/1277826 traités...\n",
      "✅ 110000/1277826 traités...\n",
      "✅ 120000/1277826 traités...\n",
      "✅ 130000/1277826 traités...\n",
      "✅ 140000/1277826 traités...\n",
      "✅ 150000/1277826 traités...\n",
      "✅ 160000/1277826 traités...\n",
      "✅ 170000/1277826 traités...\n",
      "✅ 180000/1277826 traités...\n",
      "✅ 190000/1277826 traités...\n",
      "✅ 200000/1277826 traités...\n",
      "✅ 210000/1277826 traités...\n",
      "✅ 220000/1277826 traités...\n",
      "✅ 230000/1277826 traités...\n",
      "✅ 240000/1277826 traités...\n",
      "✅ 250000/1277826 traités...\n",
      "✅ 260000/1277826 traités...\n",
      "✅ 270000/1277826 traités...\n",
      "✅ 280000/1277826 traités...\n",
      "✅ 290000/1277826 traités...\n",
      "✅ 300000/1277826 traités...\n",
      "✅ 310000/1277826 traités...\n",
      "✅ 320000/1277826 traités...\n",
      "✅ 330000/1277826 traités...\n",
      "✅ 340000/1277826 traités...\n",
      "✅ 350000/1277826 traités...\n",
      "✅ 360000/1277826 traités...\n",
      "✅ 370000/1277826 traités...\n",
      "✅ 380000/1277826 traités...\n",
      "✅ 390000/1277826 traités...\n",
      "✅ 400000/1277826 traités...\n",
      "✅ 410000/1277826 traités...\n",
      "✅ 420000/1277826 traités...\n",
      "✅ 430000/1277826 traités...\n",
      "✅ 440000/1277826 traités...\n",
      "✅ 450000/1277826 traités...\n",
      "✅ 460000/1277826 traités...\n",
      "✅ 470000/1277826 traités...\n",
      "✅ 480000/1277826 traités...\n",
      "✅ 490000/1277826 traités...\n",
      "✅ 500000/1277826 traités...\n",
      "✅ 510000/1277826 traités...\n",
      "✅ 520000/1277826 traités...\n",
      "✅ 530000/1277826 traités...\n",
      "✅ 540000/1277826 traités...\n",
      "✅ 550000/1277826 traités...\n",
      "✅ 560000/1277826 traités...\n",
      "✅ 570000/1277826 traités...\n",
      "✅ 580000/1277826 traités...\n",
      "✅ 590000/1277826 traités...\n",
      "✅ 600000/1277826 traités...\n",
      "✅ 610000/1277826 traités...\n",
      "✅ 620000/1277826 traités...\n",
      "✅ 630000/1277826 traités...\n",
      "✅ 640000/1277826 traités...\n",
      "✅ 650000/1277826 traités...\n",
      "✅ 660000/1277826 traités...\n",
      "✅ 670000/1277826 traités...\n",
      "✅ 680000/1277826 traités...\n",
      "✅ 690000/1277826 traités...\n",
      "✅ 700000/1277826 traités...\n",
      "✅ 710000/1277826 traités...\n",
      "✅ 720000/1277826 traités...\n",
      "✅ 730000/1277826 traités...\n",
      "✅ 740000/1277826 traités...\n",
      "✅ 750000/1277826 traités...\n",
      "✅ 760000/1277826 traités...\n",
      "✅ 770000/1277826 traités...\n",
      "✅ 780000/1277826 traités...\n",
      "✅ 790000/1277826 traités...\n",
      "✅ 800000/1277826 traités...\n",
      "✅ 810000/1277826 traités...\n",
      "✅ 820000/1277826 traités...\n",
      "✅ 830000/1277826 traités...\n",
      "✅ 840000/1277826 traités...\n",
      "✅ 850000/1277826 traités...\n",
      "✅ 860000/1277826 traités...\n",
      "✅ 870000/1277826 traités...\n",
      "✅ 880000/1277826 traités...\n",
      "✅ 890000/1277826 traités...\n",
      "✅ 900000/1277826 traités...\n",
      "✅ 910000/1277826 traités...\n",
      "✅ 920000/1277826 traités...\n",
      "✅ 930000/1277826 traités...\n",
      "✅ 940000/1277826 traités...\n",
      "✅ 950000/1277826 traités...\n",
      "✅ 960000/1277826 traités...\n",
      "✅ 970000/1277826 traités...\n",
      "✅ 980000/1277826 traités...\n",
      "✅ 990000/1277826 traités...\n",
      "✅ 1000000/1277826 traités...\n",
      "✅ 1010000/1277826 traités...\n",
      "✅ 1020000/1277826 traités...\n",
      "✅ 1030000/1277826 traités...\n",
      "✅ 1040000/1277826 traités...\n",
      "✅ 1050000/1277826 traités...\n",
      "✅ 1060000/1277826 traités...\n",
      "✅ 1070000/1277826 traités...\n",
      "✅ 1080000/1277826 traités...\n",
      "✅ 1090000/1277826 traités...\n",
      "✅ 1100000/1277826 traités...\n",
      "✅ 1110000/1277826 traités...\n",
      "✅ 1120000/1277826 traités...\n",
      "✅ 1130000/1277826 traités...\n",
      "✅ 1140000/1277826 traités...\n",
      "✅ 1150000/1277826 traités...\n",
      "✅ 1160000/1277826 traités...\n",
      "✅ 1170000/1277826 traités...\n",
      "✅ 1180000/1277826 traités...\n",
      "✅ 1190000/1277826 traités...\n",
      "✅ 1200000/1277826 traités...\n",
      "✅ 1210000/1277826 traités...\n",
      "✅ 1220000/1277826 traités...\n",
      "✅ 1230000/1277826 traités...\n",
      "✅ 1240000/1277826 traités...\n",
      "✅ 1250000/1277826 traités...\n",
      "✅ 1260000/1277826 traités...\n",
      "✅ 1270000/1277826 traités...\n",
      "🏁 Terminé.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:38:58.577397Z",
     "start_time": "2025-08-19T14:34:58.027404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# VAL\n",
    "df_val[\"text\"] = df_val[\"text\"].fillna(\"\").astype(str)\n",
    "df_val[\"text_clean\"] = preprocess_batch(df_val[\"text\"].tolist(), mode=\"none\")\n",
    "df_val[\"text_lemma\"] = preprocess_batch(df_val[\"text\"].tolist(), mode=\"lemma\")\n",
    "df_val[\"text_stem\"]  = preprocess_batch(df_val[\"text\"].tolist(), mode=\"stem\")\n",
    "\n"
   ],
   "id": "deedfe4733530406",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Prétraitement (none) sur 159728 textes...\n",
      "✅ 10000/159728 traités...\n",
      "✅ 20000/159728 traités...\n",
      "✅ 30000/159728 traités...\n",
      "✅ 40000/159728 traités...\n",
      "✅ 50000/159728 traités...\n",
      "✅ 60000/159728 traités...\n",
      "✅ 70000/159728 traités...\n",
      "✅ 80000/159728 traités...\n",
      "✅ 90000/159728 traités...\n",
      "✅ 100000/159728 traités...\n",
      "✅ 110000/159728 traités...\n",
      "✅ 120000/159728 traités...\n",
      "✅ 130000/159728 traités...\n",
      "✅ 140000/159728 traités...\n",
      "✅ 150000/159728 traités...\n",
      "🏁 Terminé.\n",
      "🚀 Prétraitement (lemma) sur 159728 textes...\n",
      "✅ 10000/159728 traités...\n",
      "✅ 20000/159728 traités...\n",
      "✅ 30000/159728 traités...\n",
      "✅ 40000/159728 traités...\n",
      "✅ 50000/159728 traités...\n",
      "✅ 60000/159728 traités...\n",
      "✅ 70000/159728 traités...\n",
      "✅ 80000/159728 traités...\n",
      "✅ 90000/159728 traités...\n",
      "✅ 100000/159728 traités...\n",
      "✅ 110000/159728 traités...\n",
      "✅ 120000/159728 traités...\n",
      "✅ 130000/159728 traités...\n",
      "✅ 140000/159728 traités...\n",
      "✅ 150000/159728 traités...\n",
      "🏁 Terminé.\n",
      "🚀 Prétraitement (stem) sur 159728 textes...\n",
      "✅ 10000/159728 traités...\n",
      "✅ 20000/159728 traités...\n",
      "✅ 30000/159728 traités...\n",
      "✅ 40000/159728 traités...\n",
      "✅ 50000/159728 traités...\n",
      "✅ 60000/159728 traités...\n",
      "✅ 70000/159728 traités...\n",
      "✅ 80000/159728 traités...\n",
      "✅ 90000/159728 traités...\n",
      "✅ 100000/159728 traités...\n",
      "✅ 110000/159728 traités...\n",
      "✅ 120000/159728 traités...\n",
      "✅ 130000/159728 traités...\n",
      "✅ 140000/159728 traités...\n",
      "✅ 150000/159728 traités...\n",
      "🏁 Terminé.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:43:04.916763Z",
     "start_time": "2025-08-19T14:38:58.645039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TEST\n",
    "df_test[\"text\"] = df_test[\"text\"].fillna(\"\").astype(str)\n",
    "df_test[\"text_clean\"] = preprocess_batch(df_test[\"text\"].tolist(), mode=\"none\")\n",
    "df_test[\"text_lemma\"] = preprocess_batch(df_test[\"text\"].tolist(), mode=\"lemma\")\n",
    "df_test[\"text_stem\"]  = preprocess_batch(df_test[\"text\"].tolist(), mode=\"stem\")"
   ],
   "id": "92cfbbccb86462bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Prétraitement (none) sur 159729 textes...\n",
      "✅ 10000/159729 traités...\n",
      "✅ 20000/159729 traités...\n",
      "✅ 30000/159729 traités...\n",
      "✅ 40000/159729 traités...\n",
      "✅ 50000/159729 traités...\n",
      "✅ 60000/159729 traités...\n",
      "✅ 70000/159729 traités...\n",
      "✅ 80000/159729 traités...\n",
      "✅ 90000/159729 traités...\n",
      "✅ 100000/159729 traités...\n",
      "✅ 110000/159729 traités...\n",
      "✅ 120000/159729 traités...\n",
      "✅ 130000/159729 traités...\n",
      "✅ 140000/159729 traités...\n",
      "✅ 150000/159729 traités...\n",
      "🏁 Terminé.\n",
      "🚀 Prétraitement (lemma) sur 159729 textes...\n",
      "✅ 10000/159729 traités...\n",
      "✅ 20000/159729 traités...\n",
      "✅ 30000/159729 traités...\n",
      "✅ 40000/159729 traités...\n",
      "✅ 50000/159729 traités...\n",
      "✅ 60000/159729 traités...\n",
      "✅ 70000/159729 traités...\n",
      "✅ 80000/159729 traités...\n",
      "✅ 90000/159729 traités...\n",
      "✅ 100000/159729 traités...\n",
      "✅ 110000/159729 traités...\n",
      "✅ 120000/159729 traités...\n",
      "✅ 130000/159729 traités...\n",
      "✅ 140000/159729 traités...\n",
      "✅ 150000/159729 traités...\n",
      "🏁 Terminé.\n",
      "🚀 Prétraitement (stem) sur 159729 textes...\n",
      "✅ 10000/159729 traités...\n",
      "✅ 20000/159729 traités...\n",
      "✅ 30000/159729 traités...\n",
      "✅ 40000/159729 traités...\n",
      "✅ 50000/159729 traités...\n",
      "✅ 60000/159729 traités...\n",
      "✅ 70000/159729 traités...\n",
      "✅ 80000/159729 traités...\n",
      "✅ 90000/159729 traités...\n",
      "✅ 100000/159729 traités...\n",
      "✅ 110000/159729 traités...\n",
      "✅ 120000/159729 traités...\n",
      "✅ 130000/159729 traités...\n",
      "✅ 140000/159729 traités...\n",
      "✅ 150000/159729 traités...\n",
      "🏁 Terminé.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:43:05.328875Z",
     "start_time": "2025-08-19T14:43:05.013677Z"
    }
   },
   "cell_type": "code",
   "source": "df_train[[\"text\", \"text_clean\", \"text_lemma\", \"text_stem\"]].sample(42).values",
   "id": "f6f6406b51cd9423",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ARE YOU KIDDDING? we were gonna come and seee you but we didnt have a ride  i hope you meet you tomorrrow. PLEASE COME OUTSIDE.',\n",
       "        'kiddding gon na come seee nt ride hope meet tomorrrow come outside',\n",
       "        'kiddde go to come seee not ride hope meet tomorrrow come outside',\n",
       "        'kidd gon na come seee nt ride hope meet tomorrrow come outsid'],\n",
       "       ['My Dad RULES  Hows you anyway?', 'dad rules', 'dad rule',\n",
       "        'dad rule'],\n",
       "       [\"PA only has 3 good things: my family, friends and my church. Everything else is such a downer. Don't be a Debbie Downer PA! SNL flashback\",\n",
       "        'pa good things family friends church downer don debbie downer pa snl flashback',\n",
       "        'pa good thing family friend church downer don debbie downer pa snl flashback',\n",
       "        'pa good thing famili friend church downer don debbi downer pa snl flashback'],\n",
       "       ['im sorry  i figured out what mine was. i had to vomit. TMI.',\n",
       "        'sorry figured vomit tmi', 'sorry figure vomit tmi',\n",
       "        'sorri figur vomit tmi'],\n",
       "       [\"tell me about it. this room...it's so sad\", 'tell room sad',\n",
       "        'tell room sad', 'tell room sad'],\n",
       "       ['Ah, obviously my sausage-buying timing has just been poor. Dinkie definitely looked empty though',\n",
       "        'ah obviously sausage buying timing poor dinkie definitely looked',\n",
       "        'ah obviously sausage buying timing poor dinkie definitely look',\n",
       "        'ah obvious sausag buy time poor dinki definit look'],\n",
       "       [\"24 was awesome tonight.  Next week's finale is bittersweet.  I am excited to see it but then the season is over\",\n",
       "        'awesome tonight week finale bittersweet excited season',\n",
       "        'awesome tonight week finale bittersweet excited season',\n",
       "        'awesom tonight week final bittersweet excit season'],\n",
       "       ['damn... i still have to go to school on my birthday',\n",
       "        'damn school birthday', 'damn school birthday',\n",
       "        'damn school birthday'],\n",
       "       ['n sei com que roupa vou no fashion rio',\n",
       "        'sei com que roupa vou fashion rio',\n",
       "        'sei com que roupa vou fashion rio',\n",
       "        'sei com que roupa vou fashion rio'],\n",
       "       ['yep. Just downloaded it. Updates are free too',\n",
       "        'yep downloaded updates free', 'yep download update free',\n",
       "        'yep download updat free'],\n",
       "       [\"is leaving for DC in just a few days..i can't believe that it came so quickly...still accepting donations so let me know if you have some\",\n",
       "        'leaving dc days believe came quickly accepting donations let know',\n",
       "        'leave dc day believe come quickly accept donation let know',\n",
       "        'leav dc day believ came quick accept donat let know'],\n",
       "       ['Am having fun in the sun!  Its like school didnt exist. Oh darn, i just mentioned school :l',\n",
       "        'having fun sun like school nt exist oh darn mentioned school',\n",
       "        'have fun sun like school not exist oh darn mention school',\n",
       "        'have fun sun like school nt exist oh darn mention school'],\n",
       "       ['but i dont eat kfc  save the chicken',\n",
       "        'nt eat kfc save chicken', 'not eat kfc save chicken',\n",
       "        'nt eat kfc save chicken'],\n",
       "       ['sweet! I hope our hours of work helped in some way',\n",
       "        'sweet hope hours work helped way',\n",
       "        'sweet hope hour work help way', 'sweet hope hour work help way'],\n",
       "       ['tell that continent i miss it too', 'tell continent miss',\n",
       "        'tell continent miss', 'tell contin miss'],\n",
       "       ['I think i am just destined to wear high waters. They just dont make pants for tall little people',\n",
       "        'think destined wear high waters nt pants tall little people',\n",
       "        'think destine wear high water not pant tall little people',\n",
       "        'think destin wear high water nt pant tall littl peopl'],\n",
       "       ['Cat died today.', 'cat died today', 'cat die today',\n",
       "        'cat die today'],\n",
       "       ['I &lt;3 u girls!', 'girls', 'girl', 'girl'],\n",
       "       ['ah bless craig... I seen some the other night on E4 bb docu... i felt really bad for him...',\n",
       "        'ah bless craig seen night e4 bb docu felt bad',\n",
       "        'ah bless craig see night e4 bb docu feel bad',\n",
       "        'ah bless craig seen night e4 bb docu felt bad'],\n",
       "       ['YEAHH! I know! I want a picture with him.',\n",
       "        'yeahh know want picture', 'yeahh know want picture',\n",
       "        'yeahh know want pictur'],\n",
       "       [\"Just ate ribs. I feel like a total caveman now; i'm thinking about going vegan. Nah, burgers are heaven\",\n",
       "        'ate ribs feel like total caveman thinking going vegan nah burgers heaven',\n",
       "        'eat rib feel like total caveman think go vegan nah burger heaven',\n",
       "        'ate rib feel like total caveman think go vegan nah burger heaven'],\n",
       "       [\"can't find her not-husband on twitter and is sad\",\n",
       "        'find husband twitter sad', 'find husband twitter sad',\n",
       "        'find husband twitter sad'],\n",
       "       [\"Is finally back on editing 'The Battle' After numerous teething problems\",\n",
       "        'finally editing battle numerous teething problems',\n",
       "        'finally edit battle numerous teething problem',\n",
       "        'final edit battl numer teeth problem'],\n",
       "       ['I am completely in love with gambit', 'completely love gambit',\n",
       "        'completely love gambit', 'complet love gambit'],\n",
       "       ['Final episdoe of supernatural tomorrow',\n",
       "        'final episdoe supernatural tomorrow',\n",
       "        'final episdoe supernatural tomorrow',\n",
       "        'final episdo supernatur tomorrow'],\n",
       "       ['watching the hills, woohoo!', 'watching hills woohoo',\n",
       "        'watch hill woohoo', 'watch hill woohoo'],\n",
       "       [\"I'm fully recovered after that pic!!!  I disappeared last night coz I lost the battle for the laptop   how are you? xx\",\n",
       "        'fully recovered pic disappeared night coz lost battle laptop xx',\n",
       "        'fully recover pic disappear night coz lose battle laptop xx',\n",
       "        'fulli recov pic disappear night coz lost battl laptop xx'],\n",
       "       ['On my way home for a lovely sit down...and then to do dishes',\n",
       "        'way home lovely sit dishes', 'way home lovely sit dish',\n",
       "        'way home love sit dish'],\n",
       "       ['I wanna watch that movie with you. I am kinda lonely',\n",
       "        'wanna watch movie kinda lonely',\n",
       "        'wanna watch movie kinda lonely', 'wanna watch movi kinda lone'],\n",
       "       [\"I'd love to DM you, but I can't   but check out:  http://bit.ly/CpL8L\",\n",
       "        'love dm check', 'love dm check', 'love dm check'],\n",
       "       ['hey there, your updates are coming through fine and clear.  Receiving your messages',\n",
       "        'hey updates coming fine clear receiving messages',\n",
       "        'hey update come fine clear receive message',\n",
       "        'hey updat come fine clear receiv messag'],\n",
       "       [\"I'm on my way home. What do u know about Sat OT?\",\n",
       "        'way home know sat ot', 'way home know sat ot',\n",
       "        'way home know sat ot'],\n",
       "       ['Subwaysian - Dillon - Blue Juice =',\n",
       "        'subwaysian dillon blue juice', 'subwaysian dillon blue juice',\n",
       "        'subwaysian dillon blue juic'],\n",
       "       [\"ahh I knoo are yu riding with mee?? I'm tryna get there b4 9.. I get off at 7\",\n",
       "        'ahh knoo yu riding mee tryna b4',\n",
       "        'ahh knoo yu ride mee tryna b4', 'ahh knoo yu ride mee tryna b4'],\n",
       "       ['Goodmorning', 'goodmorning', 'goodmorning', 'goodmorn'],\n",
       "       ['Ah crap. Was just getting into reading and highlighting my study notes and bam! bad old music is blasting now.',\n",
       "        'ah crap getting reading highlighting study notes bam bad old music blasting',\n",
       "        'ah crap get reading highlight study note bam bad old music blast',\n",
       "        'ah crap get read highlight studi note bam bad old music blast'],\n",
       "       [\"has created the most unhealthy meal ever  it has 3 types of cheese! it didn't need 3 types of cheese.\",\n",
       "        'created unhealthy meal types cheese didn need types cheese',\n",
       "        'create unhealthy meal type cheese didn need type cheese',\n",
       "        'creat unhealthi meal type chees didn need type chees'],\n",
       "       [\"that's mean to say.\", 'mean', 'mean', 'mean'],\n",
       "       ['if sister tomorrow was small enough, id stick in her my pocket, take her everywhere and show her off to the world!!',\n",
       "        'sister tomorrow small stick pocket world',\n",
       "        'sister tomorrow small stick pocket world',\n",
       "        'sister tomorrow small stick pocket world'],\n",
       "       ['Red Lion &amp; TBones   Band &amp;     GR8 fun!',\n",
       "        'red lion tbones band gr8 fun', 'red lion tbones band gr8 fun',\n",
       "        'red lion tbone band gr8 fun'],\n",
       "       ['hang in there..ask Him for help.', 'hang ask help',\n",
       "        'hang ask help', 'hang ask help'],\n",
       "       [\"- I'm seeing a red x, not leis.\", 'seeing red leis',\n",
       "        'see red leis', 'see red lei']], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:43:05.435297Z",
     "start_time": "2025-08-19T14:43:05.432837Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c7a0f9b9c4c88994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:44:36.602144Z",
     "start_time": "2025-08-19T14:43:05.606127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "variants = [\"text_clean\", \"text_lemma\", \"text_stem\"]\n",
    "\n",
    "for var in variants:\n",
    "    print(f\"\\n=== Logistic Regression avec {var} ===\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "    X_train = vectorizer.fit_transform(df_train[var])\n",
    "    X_val   = vectorizer.transform(df_val[var])\n",
    "    X_test  = vectorizer.transform(df_test[var])\n",
    "\n",
    "    y_train, y_val, y_test = df_train[\"label\"], df_val[\"label\"], df_test[\"label\"]\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Validation:\")\n",
    "    print(classification_report(y_val, model.predict(X_val)))\n",
    "    print(\"Test:\")\n",
    "    print(classification_report(y_test, model.predict(X_test)))\n"
   ],
   "id": "f76a900836b1c307",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression avec text_clean ===\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78     79852\n",
      "           1       0.77      0.81      0.79     79876\n",
      "\n",
      "    accuracy                           0.78    159728\n",
      "   macro avg       0.78      0.78      0.78    159728\n",
      "weighted avg       0.78      0.78      0.78    159728\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.76      0.78     79852\n",
      "           1       0.77      0.81      0.79     79877\n",
      "\n",
      "    accuracy                           0.78    159729\n",
      "   macro avg       0.78      0.78      0.78    159729\n",
      "weighted avg       0.78      0.78      0.78    159729\n",
      "\n",
      "\n",
      "=== Logistic Regression avec text_lemma ===\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77     79852\n",
      "           1       0.76      0.80      0.78     79876\n",
      "\n",
      "    accuracy                           0.78    159728\n",
      "   macro avg       0.78      0.78      0.78    159728\n",
      "weighted avg       0.78      0.78      0.78    159728\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77     79852\n",
      "           1       0.76      0.80      0.78     79877\n",
      "\n",
      "    accuracy                           0.78    159729\n",
      "   macro avg       0.78      0.78      0.78    159729\n",
      "weighted avg       0.78      0.78      0.78    159729\n",
      "\n",
      "\n",
      "=== Logistic Regression avec text_stem ===\n",
      "Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77     79852\n",
      "           1       0.77      0.80      0.78     79876\n",
      "\n",
      "    accuracy                           0.78    159728\n",
      "   macro avg       0.78      0.78      0.78    159728\n",
      "weighted avg       0.78      0.78      0.78    159728\n",
      "\n",
      "Test:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77     79852\n",
      "           1       0.77      0.80      0.78     79877\n",
      "\n",
      "    accuracy                           0.78    159729\n",
      "   macro avg       0.78      0.78      0.78    159729\n",
      "weighted avg       0.78      0.78      0.78    159729\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:44:36.755089Z",
     "start_time": "2025-08-19T14:44:36.752714Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ca6b45baff27f7df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:26:57.256096Z",
     "start_time": "2025-08-19T15:22:25.819481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ================================\n",
    "# LogReg + TF-IDF + MLflow (clean / lemma / stem)\n",
    "# ================================\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report,\n",
    "    confusion_matrix, roc_curve, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.data\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "\n",
    "# ---------- Utilitaires ----------\n",
    "def make_mlflow_safe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_safe = df.copy()\n",
    "    int_cols = df_safe.select_dtypes(include=[\"int\", \"int32\", \"int64\"]).columns\n",
    "    if len(int_cols) > 0:\n",
    "        df_safe[int_cols] = df_safe[int_cols].astype(\"float64\")\n",
    "    return df_safe\n",
    "\n",
    "def infer_safe_signature(sample_inputs: Iterable[str], model) -> mlflow.models.signature.ModelSignature:\n",
    "    sample_inputs = list(map(str, sample_inputs))\n",
    "    y_pred = model.predict(sample_inputs)\n",
    "    y_pred = np.asarray(y_pred).astype(\"float64\")\n",
    "    return infer_signature(sample_inputs, y_pred)\n",
    "\n",
    "\n",
    "# ---------- Vérifs colonnes ----------\n",
    "required_cols = [\"text_clean\", \"text_lemma\", \"text_stem\", \"label\"]\n",
    "for name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n",
    "    for col in required_cols:\n",
    "        assert col in df.columns, f\"Colonne '{col}' manquante dans df_{name}\"\n",
    "\n",
    "# ---------- Config MLflow ----------\n",
    "mlruns_path = Path(\"../mlruns\").resolve()\n",
    "mlflow.set_tracking_uri(f\"file://{mlruns_path}\")\n",
    "mlflow.set_experiment(\"logreg_baseline\")\n",
    "\n",
    "# Copies sûres (schéma MLflow)\n",
    "df_train_safe = make_mlflow_safe(df_train)\n",
    "df_val_safe   = make_mlflow_safe(df_val)\n",
    "df_test_safe  = make_mlflow_safe(df_test)\n",
    "\n",
    "# ---------- Variantes à évaluer ----------\n",
    "variants: List[Tuple[str, str]] = [\n",
    "    (\"text_clean\", \"none\"),\n",
    "    (\"text_lemma\", \"lemma\"),\n",
    "    (\"text_stem\",  \"stem\"),\n",
    "]\n",
    "\n",
    "# ---------- Boucle variantes ----------\n",
    "for text_col, variant_name in variants:\n",
    "    print(f\"\\n=== TF-IDF + LogisticRegression | variant={variant_name} | col={text_col} ===\")\n",
    "\n",
    "    # Pipeline (nouveau à chaque run)\n",
    "    vectorizer = TfidfVectorizer(max_features=20_000, ngram_range=(1, 2))\n",
    "    model = LogisticRegression(max_iter=1_000, random_state=42)\n",
    "    pipeline = make_pipeline(vectorizer, model)\n",
    "\n",
    "    # Fit\n",
    "    pipeline.fit(df_train[text_col], df_train[\"label\"])\n",
    "\n",
    "    # Évaluation\n",
    "    y_val_pred  = pipeline.predict(df_val[text_col])\n",
    "    y_test_pred = pipeline.predict(df_test[text_col])\n",
    "\n",
    "    # Probas pour ROC\n",
    "    if hasattr(pipeline, \"predict_proba\"):\n",
    "        y_val_proba  = pipeline.predict_proba(df_val[text_col])[:, 1]\n",
    "        y_test_proba = pipeline.predict_proba(df_test[text_col])[:, 1]\n",
    "    else:\n",
    "        # fallback décision fonction (rare ici)\n",
    "        y_val_proba  = pipeline.decision_function(df_val[text_col])\n",
    "        y_test_proba = pipeline.decision_function(df_test[text_col])\n",
    "\n",
    "    # Scores\n",
    "    acc_val = float(accuracy_score(df_val[\"label\"],  y_val_pred))\n",
    "    f1_val  = float(f1_score(df_val[\"label\"],        y_val_pred,  average=\"weighted\"))\n",
    "    acc_te  = float(accuracy_score(df_test[\"label\"], y_test_pred))\n",
    "    f1_te   = float(f1_score(df_test[\"label\"],       y_test_pred, average=\"weighted\"))\n",
    "    roc_val = float(roc_auc_score(df_val[\"label\"],   y_val_proba))\n",
    "    roc_te  = float(roc_auc_score(df_test[\"label\"],  y_test_proba))\n",
    "\n",
    "    print(f\"VAL  — acc={acc_val:.4f} | f1w={f1_val:.4f} | roc_auc={roc_val:.4f}\")\n",
    "    print(f\"TEST — acc={acc_te:.4f} | f1w={f1_te:.4f} | roc_auc={roc_te:.4f}\")\n",
    "\n",
    "    # Signature (échantillon cohérent avec la colonne utilisée)\n",
    "    sample_texts: List[str] = (\n",
    "        df_val[text_col]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .head(8)\n",
    "        .tolist()\n",
    "    )\n",
    "    signature = infer_safe_signature(sample_texts, pipeline)\n",
    "\n",
    "    # Dossier artefacts locaux (pour visualiser même sans MLflow UI)\n",
    "    out_dir = Path(f\"./logreg_tfidf_out/{variant_name}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ====== Artefacts : Confusion Matrix (val & test) ======\n",
    "    from itertools import product\n",
    "    for split, y_true, y_pred in [\n",
    "        (\"val\",  df_val[\"label\"].values,  y_val_pred),\n",
    "        (\"test\", df_test[\"label\"].values, y_test_pred),\n",
    "    ]:\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(cm, interpolation=\"nearest\"); plt.title(f\"Confusion matrix ({split})\"); plt.colorbar()\n",
    "        plt.xticks([0,1], [\"neg\",\"pos\"]); plt.yticks([0,1], [\"neg\",\"pos\"])\n",
    "        for i, j in product(range(2), range(2)):\n",
    "            plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "        plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
    "        fig.savefig(out_dir / f\"confusion_matrix_{split}.png\"); plt.close(fig)\n",
    "\n",
    "    # ====== Artefacts : ROC Curve (val & test) ======\n",
    "    for split, y_true, y_score, roc in [\n",
    "        (\"val\",  df_val[\"label\"].values,  y_val_proba,  roc_val),\n",
    "        (\"test\", df_test[\"label\"].values, y_test_proba, roc_te),\n",
    "    ]:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        fig = plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC={roc:.3f}\")\n",
    "        plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC curve ({split})\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(out_dir / f\"roc_curve_{split}.png\"); plt.close(fig)\n",
    "\n",
    "    # ====== Artefacts : Learning Curve (history-like plot) ======\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator=pipeline,\n",
    "        X=df_train[text_col],\n",
    "        y=df_train[\"label\"],\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "        cv=3,\n",
    "        scoring=\"f1_weighted\",\n",
    "        n_jobs=-1,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_mean = train_scores.mean(axis=1)\n",
    "    train_std  = train_scores.std(axis=1)\n",
    "    val_mean   = val_scores.mean(axis=1)\n",
    "    val_std    = val_scores.std(axis=1)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_sizes, train_mean, label=\"Train F1 (mean)\")\n",
    "    plt.fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=0.2)\n",
    "    plt.plot(train_sizes, val_mean,   label=\"CV F1 (mean)\")\n",
    "    plt.fill_between(train_sizes, val_mean-val_std, val_mean+val_std, alpha=0.2)\n",
    "    plt.xlabel(\"Training set size\"); plt.ylabel(\"F1 weighted\")\n",
    "    plt.title(\"Learning curve (3-fold CV)\")\n",
    "    plt.legend(loc=\"best\"); plt.tight_layout()\n",
    "    fig.savefig(out_dir / \"learning_curve.png\"); plt.close(fig)\n",
    "\n",
    "    # ====== Rapport texte (val & test) ======\n",
    "    report_val  = classification_report(df_val[\"label\"],  y_val_pred,  output_dict=True)\n",
    "    report_test = classification_report(df_test[\"label\"], y_test_pred, output_dict=True)\n",
    "    (out_dir / \"classification_report_val.json\").write_text(\n",
    "        pd.Series(report_val).to_json() if isinstance(report_val, dict) else str(report_val)\n",
    "    )\n",
    "    (out_dir / \"classification_report_test.json\").write_text(\n",
    "        pd.Series(report_test).to_json() if isinstance(report_test, dict) else str(report_test)\n",
    "    )\n",
    "\n",
    "    # ----- Run MLflow -----\n",
    "    with mlflow.start_run(run_name=f\"LogReg+TFIDF__{variant_name}\"):\n",
    "        # Params\n",
    "        mlflow.log_param(\"model\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        mlflow.log_param(\"max_iter\", model.max_iter)\n",
    "        mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
    "        mlflow.log_param(\"max_features\", vectorizer.max_features)\n",
    "        mlflow.log_param(\"ngram_range\", vectorizer.ngram_range)\n",
    "        mlflow.log_param(\"text_variant\", variant_name)\n",
    "        mlflow.log_param(\"text_column\", text_col)\n",
    "\n",
    "        # Inputs\n",
    "        mlflow.log_input(mlflow.data.from_pandas(df_train_safe), context=\"train\")\n",
    "        mlflow.log_input(mlflow.data.from_pandas(df_val_safe),   context=\"val\")\n",
    "        mlflow.log_input(mlflow.data.from_pandas(df_test_safe),  context=\"test\")\n",
    "\n",
    "        # Metrics\n",
    "        mlflow.log_metric(\"val_accuracy\",  acc_val)\n",
    "        mlflow.log_metric(\"val_f1_weighted\", f1_val)\n",
    "        mlflow.log_metric(\"val_roc_auc\", roc_val)\n",
    "        mlflow.log_metric(\"test_accuracy\", acc_te)\n",
    "        mlflow.log_metric(\"test_f1_weighted\", f1_te)\n",
    "        mlflow.log_metric(\"test_roc_auc\", roc_te)\n",
    "\n",
    "        # Model + signature\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=pipeline,\n",
    "            name=\"model\",\n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "        # Log des artefacts (images + rapports)\n",
    "        mlflow.log_artifacts(str(out_dir), artifact_path=f\"artifacts_{variant_name}\")\n",
    "\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        print(\n",
    "            f\"✅ MLflow run={run_id} | variant={variant_name} — \"\n",
    "            f\"val_acc={acc_val:.4f}, val_f1w={f1_val:.4f}, val_auc={roc_val:.4f} | \"\n",
    "            f\"test_acc={acc_te:.4f}, test_f1w={f1_te:.4f}, test_auc={roc_te:.4f}\"\n",
    "        )\n"
   ],
   "id": "8beece074d58a068",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF + LogisticRegression | variant=none | col=text_clean ===\n",
      "VAL  — acc=0.7810 | f1w=0.7808 | roc_auc=0.8619\n",
      "TEST — acc=0.7814 | f1w=0.7813 | roc_auc=0.8622\n",
      "✅ MLflow run=a392d153283e4e0cbff35dc9b9b6f51f | variant=none — val_acc=0.7810, val_f1w=0.7808, val_auc=0.8619 | test_acc=0.7814, test_f1w=0.7813, test_auc=0.8622\n",
      "\n",
      "=== TF-IDF + LogisticRegression | variant=lemma | col=text_lemma ===\n",
      "VAL  — acc=0.7762 | f1w=0.7761 | roc_auc=0.8577\n",
      "TEST — acc=0.7768 | f1w=0.7767 | roc_auc=0.8574\n",
      "✅ MLflow run=9c7e8ef4dfc244baace2f96551ea7423 | variant=lemma — val_acc=0.7762, val_f1w=0.7761, val_auc=0.8577 | test_acc=0.7768, test_f1w=0.7767, test_auc=0.8574\n",
      "\n",
      "=== TF-IDF + LogisticRegression | variant=stem | col=text_stem ===\n",
      "VAL  — acc=0.7795 | f1w=0.7794 | roc_auc=0.8608\n",
      "TEST — acc=0.7798 | f1w=0.7797 | roc_auc=0.8604\n",
      "✅ MLflow run=913a92ae2de94fdc8f522b5b4307d687 | variant=stem — val_acc=0.7795, val_f1w=0.7794, val_auc=0.8608 | test_acc=0.7798, test_f1w=0.7797, test_auc=0.8604\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:45:17.595070Z",
     "start_time": "2025-08-19T14:45:17.448862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_DIR = Path(\"../data\")\n",
    "train_df = pick_df(\"df_train\", DATA_DIR / \"train.csv\")\n",
    "val_df   = pick_df(\"df_val\",   DATA_DIR / \"val.csv\")\n",
    "test_df  = pick_df(\"df_test\",  DATA_DIR / \"test.csv\")"
   ],
   "id": "15ffe251e9156c30",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pick_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m DATA_DIR = Path(\u001B[33m\"\u001B[39m\u001B[33m../data\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m train_df = \u001B[43mpick_df\u001B[49m(\u001B[33m\"\u001B[39m\u001B[33mdf_train\u001B[39m\u001B[33m\"\u001B[39m, DATA_DIR / \u001B[33m\"\u001B[39m\u001B[33mtrain.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      3\u001B[39m val_df   = pick_df(\u001B[33m\"\u001B[39m\u001B[33mdf_val\u001B[39m\u001B[33m\"\u001B[39m,   DATA_DIR / \u001B[33m\"\u001B[39m\u001B[33mval.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      4\u001B[39m test_df  = pick_df(\u001B[33m\"\u001B[39m\u001B[33mdf_test\u001B[39m\u001B[33m\"\u001B[39m,  DATA_DIR / \u001B[33m\"\u001B[39m\u001B[33mtest.csv\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'pick_df' is not defined"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹DistilBERT + MLflow\n",
    "- **Modèle :** DistilBERT (`distilbert-base-uncased`), une version compacte et optimisée de BERT.\n",
    "- **Type :** Transformer pré-entraîné, spécialisé dans la compréhension du langage.\n",
    "- **Principe :** génère des embeddings contextuels pour chaque mot, puis une couche de classification binaire est entraînée par fine-tuning.\n",
    "- **Entraînement :** effectué via la classe `Trainer` de Hugging Face Transformers.\n",
    "- **Suivi :** MLflow enregistre les hyperparamètres, métriques (`accuracy`, `f1`) et sauvegarde le modèle/tokenizer.\n"
   ],
   "id": "89e90d252dcbcd84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# DistilBERT + MLflow\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
    "import transformers\n",
    "\n",
    "from scripts.p7kit import (get_paths, silent_transformers, mlflow_setup, run,\n",
    "                           log_params, log_metrics, log_inputs, make_mlflow_safe,\n",
    "                           log_artifacts_dir, prepare_out_dir, banner_ok)\n",
    "\n",
    "silent_transformers()\n",
    "NB_DIR, DATA_DIR, MLRUNS_DIR, _ = get_paths()\n",
    "\n",
    "# --- config ---\n",
    "MODEL_CKPT = \"distilbert-base-uncased\"\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 1\n",
    "TRAIN_BS = 16\n",
    "EVAL_BS = 32\n",
    "SEED = 42\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "hf_val   = Dataset.from_pandas(val_df,   preserve_index=False)\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "def tokenize(batch): return tok(batch[\"text\"], truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "hf_train_tok = hf_train.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "hf_val_tok   = hf_val.map(tokenize,   batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tok)\n",
    "id2label = {0: \"neg\", 1: \"pos\"}; label2id = {\"neg\": 0, \"pos\": 1}\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CKPT, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"eval_accuracy\": accuracy_score(labels, preds),\n",
    "            \"eval_f1_weighted\": f1_score(labels, preds, average=\"weighted\")}\n",
    "\n",
    "# --- MLflow + train ---\n",
    "mlflow_setup(MLRUNS_DIR, \"advanced_bert\")\n",
    "use_fp16 = bool(torch.cuda.is_available() and hasattr(TrainingArguments, \"fp16\"))\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_out\", seed=SEED,\n",
    "    per_device_train_batch_size=TRAIN_BS,\n",
    "    per_device_eval_batch_size=EVAL_BS,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    report_to=[],\n",
    "    fp16=use_fp16,\n",
    ")\n",
    "\n",
    "with run(run_name=f\"DistilBERT_finetune_{MODEL_CKPT}\"):\n",
    "    log_params({\"transformers_version\": transformers.__version__,\n",
    "                \"model_ckpt\": MODEL_CKPT, \"max_length\": MAX_LENGTH,\n",
    "                \"epochs\": EPOCHS, \"train_bs\": TRAIN_BS, \"eval_bs\": EVAL_BS, \"seed\": SEED, \"fp16\": use_fp16})\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=hf_train_tok, eval_dataset=hf_val_tok,\n",
    "                      processing_class=tok, data_collator=collator, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "    eval_metrics = trainer.evaluate(); log_metrics({k: float(v) for k, v in eval_metrics.items() if isinstance(v, (int, float))})\n",
    "\n",
    "    best_dir = prepare_out_dir(Path(\"./distilbert_out\"), \"best\")\n",
    "    trainer.save_model(best_dir); tok.save_pretrained(best_dir)\n",
    "    log_artifacts_dir(best_dir, \"distilbert_model\")\n",
    "    log_inputs(make_mlflow_safe(train_df), make_mlflow_safe(val_df), None)\n",
    "\n",
    "banner_ok(\"DistilBERT\", Path(\"./distilbert_out/best\"), {\"eval_accuracy\": eval_metrics.get(\"eval_accuracy\", 0.0),\n",
    "                                                        \"eval_f1_weighted\": eval_metrics.get(\"eval_f1_weighted\", 0.0)})\n"
   ],
   "id": "456840f0ee3dced9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹Keras simple (Embedding → Dense)\n",
    "- **Modèle :** réseau de neurones séquentiel avec :\n",
    "  - Une couche `Embedding` initialisée aléatoirement.\n",
    "  - Un `Flatten` suivi de deux couches `Dense`.\n",
    "  - Une sortie sigmoïde pour la classification binaire.\n",
    "- **Type :** MLP (Multi-Layer Perceptron) avec embeddings appris directement sur le dataset.\n",
    "- **Principe :** sert de baseline rapide, sans recours à des embeddings pré-entraînés.\n",
    "- **Entraînement :** via `model.fit()` en Keras, avec `EarlyStopping`.\n",
    "- **Suivi :** MLflow trace les métriques (`accuracy`, `f1`), génère un rapport de classification et une matrice de confusion, et sauvegarde modèle + tokenizer.\n"
   ],
   "id": "c6f13d94bec7934b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Keras simple (Embedding -> Flatten -> Dense)\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from scripts.p7kit import (get_paths, silent_tf, pick_or_csv, ensure_cols, ensure_int_labels,\n",
    "                           mlflow_setup, run, log_params, log_metrics, log_inputs, make_mlflow_safe,\n",
    "                           save_classification_artifacts, log_artifacts_dir, prepare_out_dir, banner_ok)\n",
    "\n",
    "silent_tf()\n",
    "NB_DIR, DATA_DIR, MLRUNS_DIR, _ = get_paths()\n",
    "\n",
    "# --- config ---\n",
    "TOP_WORDS = 10_000; MAX_LEN = 500; EMB_DIM = 32\n",
    "EPOCHS = 5; BATCH_SIZE = 128; SEED = 42\n",
    "\n",
    "# --- data (df_* si defs, sinon CSV) ---\n",
    "train_df = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_train\"), DATA_DIR/\"train.csv\")))\n",
    "val_df   = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_val\"),   DATA_DIR/\"val.csv\")))\n",
    "test_df  = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_test\"),  DATA_DIR/\"test.csv\")))\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tok = keras.preprocessing.text.Tokenizer(num_words=TOP_WORDS, oov_token=\"<unk>\")\n",
    "tok.fit_on_texts(train_df[\"text\"])\n",
    "def to_seq(s): return keras.preprocessing.sequence.pad_sequences(tok.texts_to_sequences(s), maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_tr, y_tr = to_seq(train_df[\"text\"]), train_df[\"label\"].values\n",
    "X_va, y_va = to_seq(val_df[\"text\"]),   val_df[\"label\"].values\n",
    "X_te, y_te = to_seq(test_df[\"text\"]),  test_df[\"label\"].values\n",
    "vocab_size = min(TOP_WORDS, len(tok.word_index) + 1)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=EMB_DIM),\n",
    "    layers.Flatten(), layers.Dense(16, activation=\"relu\"), layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# --- MLflow + train ---\n",
    "mlflow_setup(MLRUNS_DIR, \"keras_simple\")\n",
    "out_dir = prepare_out_dir(Path(\"./keras_simple_out\"), \"best\")\n",
    "with run(run_name=f\"keras_simple_embed{EMB_DIM}_flat_dense\"):\n",
    "    log_params({\"top_words\": TOP_WORDS, \"max_len\": MAX_LEN, \"emb_dim\": EMB_DIM,\n",
    "                \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"seed\": SEED})\n",
    "    log_inputs(make_mlflow_safe(train_df), make_mlflow_safe(val_df), make_mlflow_safe(test_df))\n",
    "\n",
    "    h = model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True, monitor=\"val_accuracy\")],\n",
    "                  verbose=1)\n",
    "\n",
    "    val_acc = float(max(h.history[\"val_accuracy\"]))\n",
    "    y_prob = model.predict(X_te, batch_size=BATCH_SIZE).ravel()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    test_metrics = {\"test_accuracy\": float(accuracy_score(y_te, y_pred)),\n",
    "                    \"test_f1_weighted\": float(f1_score(y_te, y_pred, average=\"weighted\"))}\n",
    "    log_metrics({\"val_accuracy\": val_acc, **test_metrics})\n",
    "\n",
    "    model.save(out_dir / \"model.keras\")\n",
    "    (out_dir / \"tokenizer.json\").write_text(tok.to_json())\n",
    "    save_classification_artifacts(y_te, y_pred, out_dir)\n",
    "    log_artifacts_dir(out_dir, \"keras_simple_model\")\n",
    "\n",
    "banner_ok(\"Keras simple\", out_dir, {\"val_accuracy\": val_acc, **test_metrics})\n"
   ],
   "id": "3cf96b410630f0f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =========================================\n",
    "# Compare Stemming vs Lemmatization (no Keras Tokenizer)\n",
    "# - custom tokenization + vocab + sequences\n",
    "# - same Keras model for both variants\n",
    "# =========================================\n",
    "from __future__ import annotations\n",
    "import re, os, logging, json, math, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional: MLflow logging\n",
    "USE_MLFLOW = True\n",
    "if USE_MLFLOW:\n",
    "    import mlflow\n",
    "    import mlflow.data\n",
    "\n",
    "# Silence TF logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ===== Config =====\n",
    "MAX_VOCAB = 20000\n",
    "MAX_LEN   = 100\n",
    "EMB_DIM   = 32\n",
    "EPOCHS    = 4\n",
    "BATCH_SIZE= 128\n",
    "SEED      = 42\n",
    "\n",
    "# Paths (adapt if needed)\n",
    "NB_DIR     = Path(\".\").resolve()\n",
    "DATA_DIR   = NB_DIR.parent / \"data\"\n",
    "MLRUNS_DIR = NB_DIR.parent / \"mlruns\"\n",
    "\n",
    "# ===== Reproducibility =====\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# ===== Load data (from in-memory df_* or CSV fallback) =====\n",
    "def pick_df(name: str, csv_path: Path) -> pd.DataFrame:\n",
    "    g = globals()\n",
    "    if name in g and isinstance(g[name], pd.DataFrame):\n",
    "        return g[name].copy()\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def ensure_xy(df: pd.DataFrame, text_col=\"text\", label_col=\"label\"):\n",
    "    df = df[[text_col, label_col]].rename(columns={text_col: \"text\"}).dropna().copy()\n",
    "    df[label_col] = df[label_col].astype(int)\n",
    "    return df\n",
    "\n",
    "train_df = ensure_xy(pick_df(\"df_train\", DATA_DIR/\"train.csv\"))\n",
    "val_df   = ensure_xy(pick_df(\"df_val\",   DATA_DIR/\"val.csv\"))\n",
    "test_df  = ensure_xy(pick_df(\"df_test\",  DATA_DIR/\"test.csv\"))\n",
    "\n",
    "# ===== Preprocessing: Stemming & Lemmatization =====\n",
    "# Tokenization: simple regex on letters (keeps accented letters), lowercased\n",
    "WORD_RE = re.compile(r\"[A-Za-zÀ-ÖØ-öø-ÿ]+\", re.UNICODE)\n",
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    return [w.lower() for w in WORD_RE.findall(text or \"\")]\n",
    "\n",
    "# --- Stemming (NLTK Snowball) ---\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_tokens(texts: List[str]) -> List[List[str]]:\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        toks = simple_tokenize(t)\n",
    "        out.append([stemmer.stem(tok) for tok in toks])\n",
    "    return out\n",
    "\n",
    "# --- Lemmatization (spaCy) ---\n",
    "import spacy\n",
    "# Téléchargement auto si le modèle n'est pas déjà installé\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"textcat\"])\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"textcat\"])\n",
    "\n",
    "\n",
    "def lemma_tokens(texts: List[str]) -> List[List[str]]:\n",
    "    out = []\n",
    "    for doc in nlp.pipe(texts, batch_size=256):\n",
    "        out.append([t.lemma_.lower() for t in doc if not (t.is_space or t.is_punct)])\n",
    "    return out\n",
    "\n",
    "# ===== Vocab & sequences (custom) =====\n",
    "PAD_ID = 0\n",
    "OOV_ID = 1\n",
    "\n",
    "def build_vocab(tokenized_texts: List[List[str]], max_vocab: int) -> Dict[str, int]:\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for toks in tokenized_texts:\n",
    "        cnt.update(toks)\n",
    "    # reserve PAD=0, OOV=1, start real words at 2\n",
    "    most_common = cnt.most_common(max_vocab - 2)\n",
    "    word_index = {w: i+2 for i, (w, _) in enumerate(most_common)}\n",
    "    return word_index\n",
    "\n",
    "def texts_to_sequences(tokenized_texts: List[List[str]], word_index: Dict[str,int]) -> List[List[int]]:\n",
    "    seqs = []\n",
    "    for toks in tokenized_texts:\n",
    "        seq = [word_index.get(tok, OOV_ID) for tok in toks]\n",
    "        seqs.append(seq)\n",
    "    return seqs\n",
    "\n",
    "def pad_sequences(seqs: List[List[int]], max_len: int) -> np.ndarray:\n",
    "    X = np.full((len(seqs), max_len), PAD_ID, dtype=np.int32)\n",
    "    for i, seq in enumerate(seqs):\n",
    "        s = seq[:max_len]\n",
    "        X[i, :len(s)] = s\n",
    "    return X\n",
    "\n",
    "\n",
    "def build_model(vocab_size: int, emb_dim: int, max_len: int) -> keras.Model:\n",
    "    inp = layers.Input(shape=(max_len,), dtype=\"int32\")\n",
    "    x = layers.Embedding(input_dim=vocab_size, output_dim=emb_dim)(inp)\n",
    "    x = layers.Flatten()(x)  # <= APPLIQUER la couche (appel), pas l’assigner\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    m = keras.Model(inp, out)\n",
    "    m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "\n",
    "# ===== Optional: MLflow helpers =====\n",
    "def mlflow_setup():\n",
    "    if not USE_MLFLOW: return\n",
    "    mlflow.set_tracking_uri(f\"file://{MLRUNS_DIR.resolve()}\")\n",
    "    mlflow.set_experiment(\"keras_simple_preproc\")\n",
    "\n",
    "def mlflow_log_inputs():\n",
    "    if not USE_MLFLOW: return\n",
    "    def safe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # avoid int schema complaints\n",
    "        df2 = df.copy()\n",
    "        int_cols = df2.select_dtypes(include=[\"int\", \"int32\", \"int64\"]).columns\n",
    "        if len(int_cols) > 0: df2[int_cols] = df2[int_cols].astype(\"float64\")\n",
    "        return df2\n",
    "    mlflow.log_input(mlflow.data.from_pandas(safe(train_df)), context=\"train_raw\")\n",
    "    mlflow.log_input(mlflow.data.from_pandas(safe(val_df)),   context=\"val_raw\")\n",
    "    mlflow.log_input(mlflow.data.from_pandas(safe(test_df)),  context=\"test_raw\")\n",
    "\n",
    "# ===== Run both variants =====\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "def run_experiment(name: str, tokenize_fn):\n",
    "    # 1) tokenize\n",
    "    tr_tok = tokenize_fn(train_df[\"text\"].tolist())\n",
    "    va_tok = tokenize_fn(val_df[\"text\"].tolist())\n",
    "    te_tok = tokenize_fn(test_df[\"text\"].tolist())\n",
    "\n",
    "    # 2) build vocab on train only\n",
    "    word_index = build_vocab(tr_tok, MAX_VOCAB)\n",
    "    vocab_size = max(word_index.values(), default=1) + 1  # +1 because indices start at 0\n",
    "\n",
    "    # 3) to sequences + pad\n",
    "    X_tr = pad_sequences(texts_to_sequences(tr_tok, word_index), MAX_LEN); y_tr = train_df[\"label\"].values\n",
    "    X_va = pad_sequences(texts_to_sequences(va_tok, word_index), MAX_LEN); y_va = val_df[\"label\"].values\n",
    "    X_te = pad_sequences(texts_to_sequences(te_tok, word_index), MAX_LEN); y_te = test_df[\"label\"].values\n",
    "\n",
    "    # 4) model\n",
    "    model = build_model(vocab_size, EMB_DIM, MAX_LEN)\n",
    "\n",
    "    # 5) MLflow + train\n",
    "    mlflow_setup()\n",
    "    if USE_MLFLOW:\n",
    "        run_name = f\"{name}_maxv{MAX_VOCAB}_len{MAX_LEN}_emb{EMB_DIM}\"\n",
    "        mlrun = mlflow.start_run(run_name=run_name)\n",
    "        mlflow.log_params({\n",
    "            \"variant\": name, \"lang\": \"en\",\n",
    "            \"max_vocab\": MAX_VOCAB, \"max_len\": MAX_LEN, \"emb_dim\": EMB_DIM,\n",
    "            \"epochs\": EPOCHS, \"batch_size\": BATCH_SIZE, \"seed\": SEED\n",
    "        })\n",
    "        mlflow_log_inputs()\n",
    "    else:\n",
    "        mlrun = None\n",
    "\n",
    "    cbs = [keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True, monitor=\"val_accuracy\")]\n",
    "    h = model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=cbs, verbose=1)\n",
    "\n",
    "    val_acc = float(max(h.history[\"val_accuracy\"]))\n",
    "\n",
    "    # 6) evaluation on test\n",
    "    y_prob = model.predict(X_te, batch_size=BATCH_SIZE).ravel()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    test_acc = float(accuracy_score(y_te, y_pred))\n",
    "    test_f1w = float(f1_score(y_te, y_pred, average=\"weighted\"))\n",
    "\n",
    "    # 7) artifacts (simple)\n",
    "    out_dir = NB_DIR / f\"keras_preproc_out/{name}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.save(out_dir / \"model.keras\")\n",
    "\n",
    "    # report + confusion matrix\n",
    "    report = classification_report(y_te, y_pred, output_dict=True)\n",
    "    (out_dir / \"classification_report.json\").write_text(json.dumps(report, indent=2))\n",
    "\n",
    "    cm = confusion_matrix(y_te, y_pred, labels=[0,1])\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(cm, interpolation=\"nearest\"); plt.title(f\"Confusion matrix ({name})\"); plt.colorbar()\n",
    "    plt.xticks([0,1], [\"neg\",\"pos\"]); plt.yticks([0,1], [\"neg\",\"pos\"])\n",
    "    for i, j in product(range(2), range(2)): plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
    "    fig.savefig(out_dir / \"confusion_matrix.png\"); plt.close(fig)\n",
    "\n",
    "    # 8) MLflow end\n",
    "    if USE_MLFLOW:\n",
    "        mlflow.log_metrics({\"val_accuracy\": val_acc, \"test_accuracy\": test_acc, \"test_f1_weighted\": test_f1w})\n",
    "        mlflow.log_artifacts(str(out_dir), artifact_path=f\"artifacts_{name}\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    print(f\"✅ {name} — val_acc={val_acc:.4f} | test_acc={test_acc:.4f} | test_f1w={test_f1w:.4f}\")\n",
    "    print(f\"   Saved to: {out_dir.resolve()}\")\n",
    "\n",
    "# ===== Run both: stemming and lemmatization =====\n",
    "print(f\"Language = en | MAX_VOCAB={MAX_VOCAB} | MAX_LEN={MAX_LEN} | EMB_DIM={EMB_DIM}\")\n",
    "run_experiment(\"stemming\", stem_tokens)\n",
    "run_experiment(\"lemmatization\", lemma_tokens)\n"
   ],
   "id": "323aafccce22112f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹CNN + BiLSTM (embedding pré-entraîné)\n",
    "- **Modèle :** combinaison de :\n",
    "  - **CNN 1D** : détecte des motifs locaux (n-grams) dans les textes.\n",
    "  - **BiLSTM** : capture des dépendances séquentielles dans les deux directions.\n",
    "- **Type :** réseau hybride CNN + LSTM.\n",
    "- **Embeddings :** vecteurs pré-entraînés **GloVe** (`glove.6B.100d`) ou **Twitter** (via `gensim`), utilisés en **mode gelé** (non entraînés).\n",
    "- **Principe :** exploite des représentations linguistiques déjà riches et les combine avec des couches séquentielles et convolutionnelles.\n",
    "- **Entraînement :** Keras + `EarlyStopping` et `ReduceLROnPlateau`.\n",
    "- **Suivi :** MLflow trace les métriques de validation et de test, exporte rapport et artefacts.\n"
   ],
   "id": "b021587c3134bd41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CNN + BiLSTM (embedding fixed) + tf.data\n",
    "from pathlib import Path\n",
    "import json, zipfile, requests, numpy as np, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from scripts.p7kit import (get_paths, silent_tf, pick_or_csv, ensure_cols, ensure_int_labels,\n",
    "                           mlflow_setup, run, log_params, log_metrics, log_inputs, make_mlflow_safe,\n",
    "                           save_classification_artifacts, log_artifacts_dir, prepare_out_dir, banner_ok, make_tfds)\n",
    "\n",
    "silent_tf()\n",
    "NB_DIR, DATA_DIR, MLRUNS_DIR, EMB_DIR = get_paths()\n",
    "\n",
    "# --- config ---\n",
    "EMB_NAME = \"glove.6B.100d\"  # ou \"glove-twitter-50\"\n",
    "MAX_VOCAB = 50_000; MAX_LEN = 60; BATCH_SIZE = 256; EPOCHS = 10; SEED = 42\n",
    "\n",
    "# --- data ---\n",
    "train_df = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_train\"), DATA_DIR/\"train.csv\")))\n",
    "val_df   = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_val\"),   DATA_DIR/\"val.csv\")))\n",
    "test_df  = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_test\"),  DATA_DIR/\"test.csv\")))\n",
    "y_tr, y_va, y_te = train_df[\"label\"].values, val_df[\"label\"].values, test_df[\"label\"].values\n",
    "\n",
    "tok = keras.preprocessing.text.Tokenizer(num_words=MAX_VOCAB, oov_token=\"<unk>\")\n",
    "tok.fit_on_texts(train_df[\"text\"])\n",
    "def to_seq(s): return keras.preprocessing.sequence.pad_sequences(tok.texts_to_sequences(s), maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_tr, X_va, X_te = to_seq(train_df[\"text\"]), to_seq(val_df[\"text\"]), to_seq(test_df[\"text\"])\n",
    "\n",
    "# --- embeddings (mêmes loaders, mais dans la cellule) ---\n",
    "def load_glove_6B(dim: int):\n",
    "    zip_path = EMB_DIR / \"glove.6B.zip\"\n",
    "    if not zip_path.exists():\n",
    "        r = requests.get(\"https://nlp.stanford.edu/data/glove.6B.zip\", timeout=300); r.raise_for_status()\n",
    "        zip_path.write_bytes(r.content)\n",
    "    txt_name = f\"glove.6B.{dim}d.txt\"; txt_path = EMB_DIR / txt_name\n",
    "    if not txt_path.exists():\n",
    "        with zipfile.ZipFile(zip_path) as zf: zf.extract(txt_name, EMB_DIR)\n",
    "    emb = {}\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            emb[parts[0]] = np.asarray(parts[1:], dtype=\"float32\")\n",
    "    return emb\n",
    "\n",
    "def load_glove_twitter(dim: int):\n",
    "    import gensim.downloader as api\n",
    "    return api.load(f\"glove-twitter-{dim}\")\n",
    "\n",
    "def build_embedding_matrix(emb_name: str, word_index: dict[str,int], num_words: int, seed=SEED):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    if emb_name.startswith(\"glove.6B.\"):\n",
    "        dim = int(emb_name.split(\".\")[-1][:-1]); emb_idx = load_glove_6B(dim)\n",
    "        M = rng.normal(scale=0.6, size=(num_words, dim)).astype(\"float32\")\n",
    "        for w, i in word_index.items():\n",
    "            if i < num_words:\n",
    "                v = emb_idx.get(w);  M[i] = v if v is not None else M[i]\n",
    "        return M, dim\n",
    "    else:\n",
    "        dim = int(emb_name.split(\"-\")[-1]); kv = load_glove_twitter(dim)\n",
    "        M = rng.normal(scale=0.6, size=(num_words, dim)).astype(\"float32\")\n",
    "        for w, i in word_index.items():\n",
    "            if i < num_words and w in kv: M[i] = kv[w]\n",
    "        return M, dim\n",
    "\n",
    "word_index = tok.word_index\n",
    "num_words  = min(MAX_VOCAB, len(word_index) + 1)\n",
    "emb_matrix, EMB_DIM = build_embedding_matrix(EMB_NAME, word_index, num_words)\n",
    "\n",
    "ds_tr = make_tfds(X_tr, y_tr, BATCH_SIZE, SEED, training=True)\n",
    "ds_va = make_tfds(X_va, y_va, BATCH_SIZE, SEED, training=False)\n",
    "ds_te = make_tfds(X_te, y_te, BATCH_SIZE, SEED, training=False)\n",
    "\n",
    "def build_model(num_words, emb_dim, emb_matrix, max_len):\n",
    "    inp = layers.Input(shape=(max_len,), dtype=\"int32\")\n",
    "    emb = layers.Embedding(num_words, emb_dim, weights=[emb_matrix], input_length=max_len, trainable=False)(inp)\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(emb)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    m = keras.Model(inp, out)\n",
    "    m.compile(optimizer=keras.optimizers.Adam(2e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "model = build_model(num_words, EMB_DIM, emb_matrix, MAX_LEN)\n",
    "\n",
    "# --- MLflow + train ---\n",
    "mlflow_setup(MLRUNS_DIR, \"keras_advanced_dl\")\n",
    "run_name = f\"cnn_bilstm__{EMB_NAME.replace('.','_')}\"\n",
    "out_dir  = prepare_out_dir(Path(\"./keras_adv_out\"), run_name)\n",
    "\n",
    "with run(run_name=run_name):\n",
    "    log_params({\"embedding\": EMB_NAME, \"emb_dim\": EMB_DIM, \"max_vocab\": MAX_VOCAB,\n",
    "                \"max_len\": MAX_LEN, \"batch_size\": BATCH_SIZE, \"epochs\": EPOCHS, \"seed\": SEED})\n",
    "    log_inputs(make_mlflow_safe(train_df), make_mlflow_safe(val_df), make_mlflow_safe(test_df))\n",
    "\n",
    "    h = model.fit(ds_tr, validation_data=ds_va, epochs=EPOCHS,\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "                             keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=\"val_loss\")],\n",
    "                  verbose=1)\n",
    "\n",
    "    val_acc = float(max(h.history[\"val_accuracy\"]))\n",
    "    y_pred = (model.predict(ds_te).ravel() > 0.5).astype(int)\n",
    "    test_acc = float(accuracy_score(y_te, y_pred))\n",
    "    test_f1w = float(f1_score(y_te, y_pred, average=\"weighted\"))\n",
    "    log_metrics({\"val_accuracy\": val_acc, \"test_accuracy\": test_acc, \"test_f1_weighted\": test_f1w})\n",
    "\n",
    "    model.save(out_dir / \"model.keras\")\n",
    "    (out_dir / \"tokenizer.json\").write_text(keras.preprocessing.text.tokenizer_from_json(tok.to_json()).to_json())\n",
    "    save_classification_artifacts(y_te, y_pred, out_dir)\n",
    "    log_artifacts_dir(out_dir, \"model\")\n",
    "\n",
    "banner_ok(\"CNN+BiLSTM (frozen)\", out_dir, {\"val_accuracy\": val_acc, \"test_accuracy\": test_acc, \"test_f1_weighted\": test_f1w})\n"
   ],
   "id": "b1c7facea2941423",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🔹CNN + BiLSTM (embedding trainable + régularisation)\n",
    "- **Modèle :** similaire mais avec :\n",
    "  - **Embeddings Twitter 200d** pré-entraînés et **rendus entraînables**.\n",
    "  - **SpatialDropout1D** pour régulariser l’apprentissage des séquences.\n",
    "- **Type :** réseau hybride CNN + LSTM avec régularisation avancée.\n",
    "- **Principe :** les embeddings peuvent s’adapter au corpus spécifique, ce qui augmente la flexibilité mais aussi le risque d’overfitting.\n",
    "- **Entraînement :** Keras + `EarlyStopping` et `ReduceLROnPlateau`.\n",
    "- **Suivi :** MLflow logge les mêmes éléments (métriques, rapport, matrice de confusion, modèle et tokenizer).\n"
   ],
   "id": "759fe2f9817e6249"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CNN + BiLSTM (trainable embedding + SpatialDropout)\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from scripts.p7kit import (get_paths, silent_tf, pick_or_csv, ensure_cols, ensure_int_labels,\n",
    "                           mlflow_setup, run, log_params, log_metrics, log_inputs, make_mlflow_safe,\n",
    "                           save_classification_artifacts, log_artifacts_dir, prepare_out_dir, banner_ok, make_tfds)\n",
    "\n",
    "silent_tf()\n",
    "NB_DIR, DATA_DIR, MLRUNS_DIR, EMB_DIR = get_paths()\n",
    "\n",
    "EMB_NAME = \"glove-twitter-200\"\n",
    "MAX_VOCAB = 50_000; MAX_LEN = 80; BATCH_SIZE = 256; EPOCHS = 10; SEED = 42\n",
    "\n",
    "# data\n",
    "train_df = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_train\"), DATA_DIR/\"train.csv\")))\n",
    "val_df   = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_val\"),   DATA_DIR/\"val.csv\")))\n",
    "test_df  = ensure_int_labels(ensure_cols(pick_or_csv(globals().get(\"df_test\"),  DATA_DIR/\"test.csv\")))\n",
    "y_tr, y_va, y_te = train_df[\"label\"].values, val_df[\"label\"].values, test_df[\"label\"].values\n",
    "\n",
    "tok = keras.preprocessing.text.Tokenizer(num_words=MAX_VOCAB, oov_token=\"<unk>\")\n",
    "tok.fit_on_texts(train_df[\"text\"])\n",
    "def to_seq(s): return keras.preprocessing.sequence.pad_sequences(tok.texts_to_sequences(s), maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "X_tr, X_va, X_te = to_seq(train_df[\"text\"]), to_seq(val_df[\"text\"]), to_seq(test_df[\"text\"])\n",
    "\n",
    "# twitter embeddings\n",
    "import gensim.downloader as api\n",
    "kv = api.load(\"glove-twitter-200\")\n",
    "rng = np.random.default_rng(SEED)\n",
    "num_words = min(MAX_VOCAB, len(tok.word_index) + 1)\n",
    "emb_matrix = rng.normal(scale=0.6, size=(num_words, 200)).astype(\"float32\")\n",
    "for w, i in tok.word_index.items():\n",
    "    if i < num_words and w in kv: emb_matrix[i] = kv[w]\n",
    "\n",
    "ds_tr = make_tfds(X_tr, y_tr, BATCH_SIZE, SEED, training=True)\n",
    "ds_va = make_tfds(X_va, y_va, BATCH_SIZE, SEED, training=False)\n",
    "ds_te = make_tfds(X_te, y_te, BATCH_SIZE, SEED, training=False)\n",
    "\n",
    "def build_model(num_words, emb_dim, emb_matrix, max_len):\n",
    "    inp = layers.Input(shape=(max_len,), dtype=\"int32\")\n",
    "    emb = layers.Embedding(num_words, emb_dim, weights=[emb_matrix], trainable=True)(inp)\n",
    "    x = layers.SpatialDropout1D(0.2)(emb)\n",
    "    x = layers.Conv1D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    m = keras.Model(inp, out)\n",
    "    m.compile(optimizer=keras.optimizers.Adam(2e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "model = build_model(num_words, 200, emb_matrix, MAX_LEN)\n",
    "\n",
    "# MLflow + train\n",
    "mlflow_setup(MLRUNS_DIR, \"keras_advanced_dl\")\n",
    "run_name = f\"cnn_bilstm__{EMB_NAME.replace('.','_')}__trainable\"\n",
    "out_dir  = prepare_out_dir(Path(\"./keras_adv_out\"), run_name)\n",
    "\n",
    "with run(run_name=run_name):\n",
    "    log_params({\"embedding\": EMB_NAME, \"emb_dim\": 200, \"max_vocab\": MAX_VOCAB,\n",
    "                \"max_len\": MAX_LEN, \"batch_size\": BATCH_SIZE, \"epochs\": EPOCHS,\n",
    "                \"seed\": SEED, \"embedding_trainable\": True, \"spatial_dropout\": 0.2})\n",
    "    log_inputs(make_mlflow_safe(train_df), make_mlflow_safe(val_df), make_mlflow_safe(test_df))\n",
    "\n",
    "    h = model.fit(ds_tr, validation_data=ds_va, epochs=EPOCHS,\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True, monitor=\"val_accuracy\"),\n",
    "                             keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.5, monitor=\"val_loss\")],\n",
    "                  verbose=1)\n",
    "\n",
    "    val_acc = float(max(h.history[\"val_accuracy\"]))\n",
    "    y_pred = (model.predict(ds_te).ravel() > 0.5).astype(int)\n",
    "    test_acc = float(accuracy_score(y_te, y_pred))\n",
    "    test_f1w = float(f1_score(y_te, y_pred, average=\"weighted\"))\n",
    "    log_metrics({\"val_accuracy\": val_acc, \"test_accuracy\": test_acc, \"test_f1_weighted\": test_f1w})\n",
    "\n",
    "    model.save(out_dir / \"model.keras\")\n",
    "    (out_dir / \"tokenizer.json\").write_text(keras.preprocessing.text.tokenizer_from_json(tok.to_json()).to_json())\n",
    "    save_classification_artifacts(y_te, y_pred, out_dir)\n",
    "    log_artifacts_dir(out_dir, \"model\")\n",
    "\n",
    "banner_ok(\"CNN+BiLSTM (trainable)\", out_dir, {\"val_accuracy\": val_acc, \"test_accuracy\": test_acc, \"test_f1_weighted\": test_f1w})\n"
   ],
   "id": "40801e957b6e1134",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "64ca0cb3e89bc932",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
