# Projet 7 â€” Analyse de sentiments via Deep Learning (Guide Markdown)

> **But** : PrÃ©parer Azure ML (CLI, workspace, compute), enregistrer les datasets, crÃ©er lâ€™environnement dâ€™exÃ©cution, lancer un job, puis dÃ©marrer une API locale avec FastAPI.
>
> **Ã€ adapter** : Remplacez les valeurs entre chevrons `<>` si nÃ©cessaire. Les commandes sont prÃ©vues pour **WSL/Linux**.

---

## 1) PrÃ©â€‘requis & Installation dâ€™Azure CLI

```bash
# 1.1 Installer curl et utilitaires
sudo apt-get update
sudo apt-get install -y curl ca-certificates gnupg lsb-release

# 1.2 Installer lâ€™Azure CLI
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# 1.3 VÃ©rifier et ajouter lâ€™extension Azure ML
az version
az extension add -n ml -y
```

### Connexion & variables dâ€™environnement (WSL)

```bash
# 1.4 Se connecter (ouvre un navigateur)
az login

# 1.5 DÃ©finir les variables (Ã  personnaliser si besoin)
SUB="<SUBSCRIPTION_ID>"   # astuce: az account show --query id -o tsv
RG="rg-p7"
WS="ws-p7"
LOCATION="francecentral"

# 1.6 SÃ©lectionner la souscription
az account set --subscription "$SUB"
```

---

## 2) VÃ©rifications rapides & Workspace

```bash
# 2.1 VÃ©rifier le compte et lâ€™extension ML
az account show -o table
az extension show -n ml || az extension add -n ml -y

# 2.2 VÃ©rifier lâ€™existence du workspace
az ml workspace show -g "$RG" -w "$WS" -o table
```

---

## 3) CrÃ©er un cluster de calcul (CPU, gratuit au repos)

> **Note** : Ajustez la taille si nÃ©cessaire. Ici, `Standard_DS2_v2`.

```bash
az ml compute create -g "$RG" -w "$WS" \
  --name cpu-cluster \
  --type amlcompute \
  --min-instances 0 --max-instances 1 \
  --size Standard_DS2_v2
```

---

## 4) Enregistrer les datasets (train/val/test)

```bash
# 4.1 VÃ©rifier la prÃ©sence des fichiers
ds_root="/projet7/data"  # adapter si besoin
ls -lh "$ds_root"/{train.csv,val.csv,test.csv}

# 4.2 CrÃ©er les assets de donnÃ©es (v1)
az ml data create -g "$RG" -w "$WS" --name sentiment140-train --version 1 \
  --type uri_file --path "$ds_root/train.csv"

az ml data create -g "$RG" -w "$WS" --name sentiment140-val --version 1 \
  --type uri_file --path "$ds_root/val.csv"

az ml data create -g "$RG" -w "$WS" --name sentiment140-test --version 1 \
  --type uri_file --path "$ds_root/test.csv"
```

---

## 5) CrÃ©er lâ€™environnement Azure ML

```bash
# 5.1 CrÃ©er lâ€™asset dâ€™environnement Ã  partir dâ€™un YAML
az ml environment create -g "$RG" -w "$WS" -f /projet7/azureml/env/environment.yml

# 5.2 VÃ©rifier lâ€™environnement (ex: p7-bert-env v1)
az ml environment show -g "$RG" -w "$WS" --name p7-bert-env --version 1 -o table
```

---

## 6) Lancer un job Azure ML

```bash
# 6.1 Se placer dans le dossier des jobs
cd /projet7/azureml/jobs

# 6.2 CrÃ©er le job Ã  partir de job.yml et rÃ©cupÃ©rer son nom
JOB=$(az ml job create -g "$RG" -w "$WS" -f job.yml --query name -o tsv)

# 6.3 Suivre les logs du job
az ml job stream -g "$RG" -w "$WS" -n "$JOB"
```

---

## 7) API locale (FastAPI) â€” Optionnel

```bash
# 7.1 CrÃ©er et activer un environnement Conda
conda create -n sentiment-api python=3.11 -y
conda activate sentiment-api

# 7.2 Installer les dÃ©pendances
pip install --upgrade pip
pip install fastapi "uvicorn[standard]"
pip install scikit-learn joblib numpy
pip install -U pytest httpx fastapi pydantic

# 7.3 DÃ©marrer lâ€™API (adapter le chemin si besoin)
cd ~/projet7  # <â€” changez ce chemin si nÃ©cessaire
uvicorn app.main:app --reload --port 8000
```

---

### Notes

* **Droits & rÃ´les** : assurez-vous dâ€™avoir les droits sur la souscription et le resource group.
* **Quotas** : si la crÃ©ation du compute Ã©choue, vÃ©rifiez les quotas de la rÃ©gion `francecentral`.
* **Chemins** : adaptez les chemins (datasets, YAMLs) Ã  votre arborescence locale.



# DÃ©ploiement Azure â€” Commande de dÃ©marrage & Git LFS

Ce mÃ©mo documente **la commande de dÃ©marrage Azure App Service** et **lâ€™usage de Git LFS** pour versionner un modÃ¨le (ex. `model.keras`) et ses artefacts (ex. `tokenizer.json`) sans casser le dÃ©ploiement.

---

## 1) Azure : dÃ©finir la commande de dÃ©marrage


### Portail Azure
App Service â†’ *Configuration* â†’ *ParamÃ¨tres gÃ©nÃ©raux* â†’ **Commande de dÃ©marrage** :
```bash
gunicorn -c gunicorn.conf.py app.main:app
```

### Azure CLI (Ã©quivalent)
```bash
az webapp config set   --resource-group <RG>   --name <APP_NAME>   --startup-file "gunicorn -c gunicorn.conf.py app.main:app"
```

---

## 2) Installer Git LFS (pour versionner les artefacts lourds)

### Ubuntu / WSL
```bash
sudo apt update
sudo apt install -y git-lfs
git lfs version   # vÃ©rif
git lfs install   # initialisation
```

> âš ï¸ GitHub refuse les fichiers > 100 Mo hors LFS.

---

## 3) Suivre et inclure les artefacts du modÃ¨le

### 3.1 Traquer les fichiers avec LFS
```bash
# Tracker les fichiers lourds
git lfs track "app/artifacts/model.keras"
git add .gitattributes    # ajoute/maj le fichier gÃ©nÃ©rÃ© par LFS
```

# Configuration Azure App Service (Linux)

Cette API FastAPI est dÃ©ployÃ©e sur **Azure App Service**. Quelques **paramÃ¨tres dâ€™application** (variables dâ€™environnement) doivent Ãªtre configurÃ©s dans le portail Azure.

## OÃ¹ configurer
**Portail Azure â†’ App Service â†’ Configuration â†’ ParamÃ¨tres dâ€™application â†’ Nouveau paramÃ¨tre**  
Renseignez les paires `Nom` / `Valeur`, **Enregistrez**, puis **RedÃ©marrez** lâ€™application.

## ParamÃ¨tres importants

| Nom (clÃ©)                             | Valeur conseillÃ©e | RÃ´le |
|--------------------------------------|-------------------|------|
| `SEQ_LEN` *(recommandÃ©)*             | `80`              | **Longueur de sÃ©quence attendue par le modÃ¨le** (en **tokens**, pas en caractÃ¨res). Doit **correspondre exactement** Ã  lâ€™input du modÃ¨le Keras (ex. `model.input_shape[1] == 80`). |
| `MAX_LEN` *(hÃ©ritage)*               | `80`              | Ancien nom utilisÃ© par certains scripts. Ã€ Ã©viter si `SEQ_LEN` est prÃ©sent. Si vous conservez `MAX_LEN`, mettez **la mÃªme valeur que lâ€™input du modÃ¨le**. |
| `CORS_ALLOW_ORIGINS`                 | `*` ou domaines   | Autorise les origines front qui appellent lâ€™API. |
| `SCM_DO_BUILD_DURING_DEPLOYMENT`     | `1`               | Laisse Oryx installer les dÃ©pendances lors du dÃ©ploiement GitHub Actions. |

### ParamÃ¨tres optionnels (modÃ¨le volumineux)
| Nom                                   | Valeur            | Pourquoi |
|---------------------------------------|-------------------|----------|
| `WEBSITES_CONTAINER_START_TIME_LIMIT` | `600`             | Accorde plus de temps au conteneur pour dÃ©marrer si le premier chargement est long. |
| `WORKERS`                             | `1`               | Ã‰vite de charger le modÃ¨le en double dans plusieurs workers Gunicorn. |
| `TIMEOUT` / `GRACEFUL_TIMEOUT`        | `300`             | DÃ©lai Gunicorn plus large pendant lâ€™infÃ©rence. |

> ðŸ’¡ **Important : `SEQ_LEN` / `MAX_LEN` sont en *tokens*** (sÃ©quences aprÃ¨s `tokenizer.texts_to_sequences`), **pas en caractÃ¨res**.  
> Exemple : si le modÃ¨le a Ã©tÃ© entraÃ®nÃ© avec `maxlen=80`, dÃ©finir `255` provoquera une erreur du type  
> `ValueError: expected shape=(None, 80), found shape=(1, 255)`.

## DÃ©terminer la bonne longueur
- La valeur cible est gÃ©nÃ©ralement celle utilisÃ©e Ã  lâ€™entraÃ®nement (`pad_sequences(..., maxlen=80)`).
- Elle peut aussi Ãªtre lue dans le modÃ¨le : `model.input_shape[1]`.

## VÃ©rifier
```bash
# SantÃ©
curl -i https://<votre-app>.azurewebsites.net/health

# PrÃ©diction (le 1er appel peut Ãªtre un peu plus lent : chargement lazy)
curl -s -X POST "https://<votre-app>.azurewebsites.net/predict" \
  -H "Content-Type: application/json" \
  -d '{"text":"Jâ€™adore ce produit, câ€™est excellent !"}'


BASE="https://p7-sentiment-api.azurewebsites.net"

# Doit rÃ©pondre: {"status":"stored"}
curl -i -X POST "$BASE/feedback" \
  -H "Content-Type: application/json" \
  -d '{"text":"Test bad prediction","predicted":"pos","correct":false,"note":"offensive content misclassified"}'
  
# Lancer les tests unitaires
pytest --cache-clear -q